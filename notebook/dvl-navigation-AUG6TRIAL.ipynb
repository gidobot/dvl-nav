{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVL Navigation and State Estimation\n",
    "\n",
    "1. [Import Libraries](#import-libraries)\n",
    "1. [Load and Parse Data](#load-and-parse-data)\n",
    "    1. [Glider Flight Computer Data](#import-glider-flight-computer)\n",
    "    1. [DVL Data](#import-dvl-data)\n",
    "    1. [Bathymetry Data](#import-bathymetry)\n",
    "    1. [Select Time Series](#import-select-time-series)\n",
    "    1. [Time Synchronization Fix](#import-time-synchronization-fix)\n",
    "1. [Compute Water Column Currents](#compute-water-column-currents)\n",
    "1. [Compute DVL-Odometry](#compute-dvl-odometry)\n",
    "1. [Multi-Factor Terrain Based Navigation](#multi-factor-terrain-based-navigation)\n",
    "    1. [Plot Navigation Results](#plot-navigation-results)\n",
    "    1. [Navigation Performance](#navigation-performance)\n",
    "        2. [Print Performance metrics](#print_metrics)\n",
    "1. [Sandbox](#sandbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='import-libraries'></a>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "#import earthpy as et\n",
    "#import earthpy.plot as ep\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rasterio as rio\n",
    "import scipy\n",
    "import scipy.signal\n",
    "import seaborn as sns \n",
    "import struct\n",
    "import sys\n",
    "import utm\n",
    "import unittest\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt \n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy import interpolate\n",
    "\n",
    "# add parent directory to the path for importing modules \n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "sys.path.append(os.path.join(sys.path[0], '../data'))\n",
    "\n",
    "# objects for parsing raw DVL data \n",
    "import PathfinderDVL\n",
    "import PathfinderEnsemble\n",
    "import PathfinderTimeSeries\n",
    "\n",
    "# objects for estimating ocean current velocities\n",
    "import VelocityShearPropagation\n",
    "\n",
    "# objects for controlling thruster to minimize transport cost \n",
    "import AdaptiveVelocityController\n",
    "\n",
    "# objects for parsing flight and science computer log files\n",
    "import SlocumFlightController\n",
    "import SlocumScienceController\n",
    "import dvl_plotter\n",
    "import BathymetryMap\n",
    "import MultiFactorTAN\n",
    "\n",
    "# data for parsing seafloor bathymetry\n",
    "import bathy_meta_data\n",
    "sns.set()\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "def reload_modules():\n",
    "    importlib.reload(PathfinderDVL)\n",
    "    importlib.reload(PathfinderEnsemble)\n",
    "    importlib.reload(PathfinderTimeSeries)\n",
    "    importlib.reload(VelocityShearPropagation)\n",
    "    importlib.reload(AdaptiveVelocityController)\n",
    "    importlib.reload(SlocumFlightController)\n",
    "    importlib.reload(SlocumScienceController)\n",
    "    importlib.reload(dvl_plotter)\n",
    "    importlib.reload(bathy_meta_data)\n",
    "    importlib.reload(BathymetryMap)\n",
    "    importlib.reload(MultiFactorTAN)\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='load-and-parse-data'></a>\n",
    "## Load and/or Parse Data\n",
    "<a id='import-glider-flight-computer'></a>\n",
    "### A. Glider Flight Computer Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Parsing folder of ASC Files\n",
      ">> Finished Parsing!\n"
     ]
    }
   ],
   "source": [
    "reload_modules()\n",
    "directory = \"C:/Users/grego/Dropbox/EPIC-DAUG/Field Ops/Summer 2021 Data/06 AUGUST SEA TRIAL/Flight/dbd_parsed/\"\n",
    "ts_flight_kolumbo_all = SlocumFlightController.SlocumFlightController.from_directory(directory, save=False, verbose=False)\n",
    "#ts_flight_kolumbo_all.df.to_csv('DBD_6AUG_PROCESSED.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Switch flight computer to local time?\n",
    "for t in range(len(ts_flight_kolumbo_all.df.time)):\n",
    "    ts_flight_kolumbo_all.df.time[t] = ts_flight_kolumbo_all.df.time[t] - (3600*4)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2021-08-06 16:05:45.050320    1.628208e+09\n",
       "2021-08-06 16:06:09.178920    1.628208e+09\n",
       "2021-08-06 16:06:19.619020    1.628208e+09\n",
       "2021-08-06 16:06:24.017180    1.628208e+09\n",
       "2021-08-06 16:06:28.449010    1.628208e+09\n",
       "                                  ...     \n",
       "2021-08-06 19:02:25.545290    1.628219e+09\n",
       "2021-08-06 19:02:29.777160    1.628219e+09\n",
       "2021-08-06 19:02:34.320800    1.628219e+09\n",
       "2021-08-06 19:02:38.829500    1.628219e+09\n",
       "2021-08-06 19:02:43.524690    1.628219e+09\n",
       "Name: time, Length: 2246, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_flight_kolumbo_all.df.time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='import-dvl-data'></a>\n",
    "### B. DVL data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________\n",
      "- Parsing DVL File ---------------------\n",
      "    input file: uh061244.pd0\n",
      "- Parsing Complete ---------------------\n",
      "    # ensembles:     28\n",
      "    parsing time:  0.062999\n",
      "- Sensor Configuration -----------------\n",
      "    600kHz System\n",
      "    Convex Beam Pattern\n",
      "    Sensor Config #1\n",
      "    Attached\n",
      "    Down Facing\n",
      "    30E Beam Angle\n",
      "    4 Beam Janus\n",
      "- Coordinate Transformation ------------\n",
      "    Bin Mapping Used\n",
      "    3-Beam Soln Used\n",
      "    Tilts Not Used\n",
      "    Instrument Coords\n",
      "________________________________________\n",
      "- Parsing DVL File ---------------------\n",
      "    input file: uh061252.pd0\n",
      "- Parsing Complete ---------------------\n",
      "    # ensembles:     14\n",
      "    parsing time:  0.030999\n",
      "- Sensor Configuration -----------------\n",
      "    600kHz System\n",
      "    Convex Beam Pattern\n",
      "    Sensor Config #1\n",
      "    Attached\n",
      "    Down Facing\n",
      "    30E Beam Angle\n",
      "    4 Beam Janus\n",
      "- Coordinate Transformation ------------\n",
      "    Bin Mapping Used\n",
      "    3-Beam Soln Used\n",
      "    Tilts Not Used\n",
      "    Instrument Coords\n",
      "________________________________________\n",
      "- Parsing DVL File ---------------------\n",
      "    input file: uh061333.pd0\n",
      "    # ensembles:    200\n",
      "    # ensembles:    400\n",
      "    # ensembles:    600\n",
      "    # ensembles:    800\n",
      "    # ensembles:   1000\n",
      "- Parsing Complete ---------------------\n",
      "    # ensembles:   1001\n",
      "    parsing time:  1.735749\n",
      "- Sensor Configuration -----------------\n",
      "    600kHz System\n",
      "    Convex Beam Pattern\n",
      "    Sensor Config #1\n",
      "    Attached\n",
      "    Down Facing\n",
      "    30E Beam Angle\n",
      "    4 Beam Janus\n",
      "- Coordinate Transformation ------------\n",
      "    Bin Mapping Used\n",
      "    3-Beam Soln Used\n",
      "    Tilts Not Used\n",
      "    Instrument Coords\n"
     ]
    }
   ],
   "source": [
    "#Aug 6 Field Trial\n",
    "\n",
    "filepath = \"C:/Users/grego/Dropbox/EPIC-DAUG/Field Ops/Summer 2021 Data/06 AUGUST SEA TRIAL/Science/pd0_raw/\"\n",
    "\n",
    "#################################################\n",
    "# File ID Number ################################\n",
    "#################################################\n",
    "filename1 = \"uh061244.pd0\"\n",
    "filename2 = \"uh061252.pd0\"\n",
    "filename3 = \"uh061333.pd0\"\n",
    "\n",
    "#################################################\n",
    "# Parse Selected File IDs #######################\n",
    "#################################################\n",
    "ts1 = PathfinderTimeSeries.PathfinderTimeSeries.from_pd0(filepath+filename1,  save=False)\n",
    "ts2 = PathfinderTimeSeries.PathfinderTimeSeries.from_pd0(filepath+filename2,  save=False)\n",
    "ts3 = PathfinderTimeSeries.PathfinderTimeSeries.from_pd0(filepath+filename3,  save=False)\n",
    "\n",
    "#################################################\n",
    "# Naming Convention #############################\n",
    "#################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='import-bathymetry'></a>\n",
    "### C. Seafloor Bathymetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_modules()\n",
    "#Grid_res_num should always be 10 to represent original resolution of the bathymetry chart. Then, the Minimum spatial resolution will be 5m\n",
    "\n",
    "#10m resolution\n",
    "bathy_df = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/sensitivity_tests/Kolumbo-1.csv')\n",
    "grid_res_num = 10\n",
    "# map_var_resolution = '10m'\n",
    "\n",
    "#20m resolution\n",
    "bathy_df_var = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/sensitivity_tests/Kolumbo-2.csv')\n",
    "map_var_resolution = '20m'\n",
    "# grid_res_num = 20\n",
    "\n",
    "\n",
    "#Legacy Stuff\n",
    "# meta_dict = bathy_meta_data.BathyData[\"Kolumbo_full\"]\n",
    "# meta_dict = bathy_meta_data.BathyData[\"Kolumbo\"]\n",
    "# bathy     = BathymetryMap.BathymetryMap(meta_dict=meta_dict)\n",
    "# bathy.parse_bathy_file()\n",
    "\n",
    "\n",
    "# bathy_df = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/Kolumbo-10m-utm-sub.csv')\n",
    "# bathy_df = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/Kolumbo-10m-sub-withVar.csv')\n",
    "\n",
    "# bathy_df = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/Kolumbo-10m-sub-low-res-test.csv')\n",
    "# bathy_df_var = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/Kolumbo-10m-sub-low-res-test-var.csv')\n",
    "\n",
    "# #Test 04 with orientation normalzied to 0 -360 deg\n",
    "# bathy_df = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/Kolumbo-10m-sub-test04.csv')\n",
    "# bathy_df_var = pd.read_csv('C:/Users/grego/Dropbox/Kolumbo cruise 2019/zduguid/bathy/Kolumbo-10m-sub-test04-var.csv')\n",
    "# print('Finished loading bathymetry data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import-select-time-series'></a>\n",
    "### D. Select Time Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select Time Series to Run Naviagation Code on\n",
    "#ts_label is purely for visual results at end (not important for code)\n",
    "ts = ts3\n",
    "dive_label = 'main'\n",
    "#ts.df.to_csv('AUG06_pd0_main.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the relevant portion of the glider flight computer\n",
    "start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "dur     = end_t - start_t \n",
    "########################TODO\n",
    "df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 8, 6, 13, 33, 39, 33)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2021, 8, 6, 14, 55, 43, 86)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.timedelta(seconds=4924, microseconds=53)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='import-time-synchronization-fix'></a>\n",
    "### D. Time Synchronization Fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract the relevant portion of the glider flight computer\n",
    "#start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "#end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "#dur     = end_t - start_t \n",
    "########################TODO\n",
    "#df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()\n",
    "\n",
    "RAD_TO_DEG = 180/np.pi\n",
    "new_pitch_list = [] \n",
    "new_roll_list = []\n",
    "new_heading_list = []\n",
    "\n",
    "for t in range(len(ts.df)):\n",
    "    # extract timestamp from PD0 file and corresponding sections of DBD file\n",
    "    time   = ts.df.time[t]\n",
    "    target = datetime.datetime.fromtimestamp(time)\n",
    "    lower  = df_dbd[:str(target)]\n",
    "    upper  = df_dbd[str(target):]\n",
    "    \n",
    "    # handle edge cases when interpolation is not possible  \n",
    "    if len(lower)==0:\n",
    "        new_pitch_list.append(upper.m_pitch[0]*RAD_TO_DEG)\n",
    "        new_roll_list.append(upper.m_roll[0]*RAD_TO_DEG)\n",
    "        new_heading_list.append(upper.m_heading[0]*RAD_TO_DEG)\n",
    "        continue\n",
    "    if len(upper)==0:\n",
    "        new_pitch_list.append(lower.m_pitch[-1]*RAD_TO_DEG)\n",
    "        new_roll_list.append(lower.m_roll[-1]*RAD_TO_DEG)\n",
    "        new_heading_list.append(lower.m_heading[-1]*RAD_TO_DEG)\n",
    "        continue \n",
    "\n",
    "    lower_time  = lower.time[-1]\n",
    "    lower_pitch = lower.m_pitch[-1]\n",
    "    lower_roll  = lower.m_roll[-1]\n",
    "    lower_heading   = lower.m_heading[-1]\n",
    "    upper_time  = upper.time[0]\n",
    "    upper_pitch = upper.m_pitch[0]\n",
    "    upper_roll  = upper.m_roll[0]\n",
    "    upper_heading   = upper.m_heading[0]\n",
    "    delta_t     = upper_time-lower_time\n",
    "    \n",
    "    # case when DBD data is repeated in successive timestamps\n",
    "    if delta_t == 0:\n",
    "        new_pitch_list.append(lower.m_pitch[-1]*RAD_TO_DEG)\n",
    "        new_roll_list.append(lower.m_roll[-1]*RAD_TO_DEG)\n",
    "        new_heading_list.append(lower.m_heading[-1]*RAD_TO_DEG)\n",
    "        continue\n",
    "        \n",
    "    # take linear interpolation between DBD values\n",
    "    lower_per   = (time-lower_time)/delta_t\n",
    "    upper_per   = 1 - lower_per\n",
    "    new_pitch   = lower_pitch*lower_per + upper_pitch*upper_per\n",
    "    new_pitch_list.append(new_pitch*RAD_TO_DEG)\n",
    "    new_roll   = lower_roll*lower_per + upper_roll*upper_per\n",
    "    new_roll_list.append(new_roll*RAD_TO_DEG)\n",
    "    new_heading   = lower_heading*lower_per + upper_heading*upper_per\n",
    "    new_heading_list.append(new_heading*RAD_TO_DEG)\n",
    "    \n",
    "ts.df['pitch'] = new_pitch_list\n",
    "ts.df['roll']  = new_roll_list\n",
    "ts.df['heading']   = new_heading_list\n",
    "print('> New pitch, roll, and heading data extracted!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='compute-water-column-currents'></a>\n",
    "## Compute Water Column Currents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reload_modules()\n",
    "\n",
    "# tuning parameters for working with DVL data \n",
    "pitch_bias           =  8    # [deg]   mounting pitch bias for the sonar\n",
    "start_filter         =  2    # [bin #] avoid using the first number of bins\n",
    "end_filter           =  2    # [bin #] avoid using the last number of bins \n",
    "voc_mag_filter       =  1.0  # [m/s]   filter out ocean current \n",
    "voc_delta_mag_filter =  0.5  # [m/s]   filter out deltas between layers\n",
    "near_surface_filter  = 10    # [m]     ignore Vtw when near surface \n",
    "\n",
    "# constants\n",
    "DEG_TO_RAD = np.pi/180\n",
    "\n",
    "# determine DVL parameters \n",
    "bin_len      = ts.df.depth_bin_length[0]\n",
    "bin0_dist    = ts.df.bin0_distance[0]\n",
    "bin_len      = np.cos(pitch_bias*DEG_TO_RAD)*bin_len\n",
    "bin0_dist    = np.cos(pitch_bias*DEG_TO_RAD)*bin0_dist\n",
    "max_range    = 80\n",
    "max_depth    = int(np.max(ts.df.depth)+80)\n",
    "x_beam       = 0\n",
    "y_beam       = 1\n",
    "\n",
    "# intialize water column\n",
    "water_column = VelocityShearPropagation.WaterColumn(\n",
    "    bin_len=bin_len, \n",
    "    bin0_dist=bin0_dist,\n",
    "    max_depth=max_depth,\n",
    "    start_filter=start_filter,\n",
    "    end_filter=end_filter,\n",
    "    voc_mag_filter=voc_mag_filter,\n",
    "    voc_delta_mag_filter=voc_delta_mag_filter,\n",
    ")\n",
    "\n",
    "# iterate over the DVL ensembles \n",
    "for t in range(len(ts.df)):\n",
    "\n",
    "    # only use Vtw from pressure sensor when submerged \n",
    "    depth = ts.df.depth[t]\n",
    "    pitch = ts.df.pitch[t]\n",
    "    roll  = ts.df.roll[t]\n",
    "    if depth > near_surface_filter:\n",
    "        vtw_u = ts.df.rel_vel_pressure_u[t]\n",
    "        vtw_v = ts.df.rel_vel_pressure_v[t]\n",
    "        \n",
    "    # otherwise use the DVL to estimate the Vtw at the surface\n",
    "    else:\n",
    "        vtw_u = ts.df.rel_vel_dvl_u[t]\n",
    "        vtw_v = ts.df.rel_vel_dvl_v[t]\n",
    "    \n",
    "    # extract Voc reference from bottom track velocity when available\n",
    "    if not np.isnan(ts.df.abs_vel_btm_u[t]):\n",
    "        vog_u = ts.df.abs_vel_btm_u[t]\n",
    "        vog_v = ts.df.abs_vel_btm_v[t]\n",
    "        voc_u = vog_u - vtw_u\n",
    "        voc_v = vog_v - vtw_v\n",
    "        voc_ref = VelocityShearPropagation.OceanCurrent(voc_u, voc_v, 0)\n",
    "    else:\n",
    "        voc_ref = VelocityShearPropagation.OceanCurrent()\n",
    "        \n",
    "    # add shear nodes for each DVL depth bin that meet the filter criteria\n",
    "    num_good_vel_bins = ts.df.num_good_vel_bins[t]\n",
    "    if num_good_vel_bins > start_filter+end_filter:        \n",
    "        \n",
    "        # determine if glider ascending or descending\n",
    "        delta_z = ts.df.delta_z[t]\n",
    "        if delta_z > 0:\n",
    "            direction = 'descending'\n",
    "        else:\n",
    "            direction = 'ascending'\n",
    "\n",
    "        # build list of velocity shears to add as ShearNode to water column\n",
    "        delta_voc_u = []\n",
    "        delta_voc_v = []\n",
    "\n",
    "        # add all valid DVL bins to the shear list \n",
    "        #   + filtering of DVL bins will occur in the `add_shear_node` call\n",
    "        for bin_num in range(int(num_good_vel_bins)):\n",
    "\n",
    "            # retrieve the shear list from the DVL data \n",
    "            x_var = ts.get_profile_var_name('velocity', bin_num, x_beam)\n",
    "            y_var = ts.get_profile_var_name('velocity', bin_num, y_beam)\n",
    "            dvl_x = ts.df[x_var][t]\n",
    "            dvl_y = ts.df[y_var][t]\n",
    "\n",
    "            # compute delta between dead-reckoned through-water velocity & DVL\n",
    "            delta_voc_u.append(vtw_u - (-dvl_x))\n",
    "            delta_voc_v.append(vtw_v - (-dvl_y))\n",
    "\n",
    "        shear_list = [VelocityShearPropagation.OceanCurrent(\n",
    "                        delta_voc_u[i], \n",
    "                        delta_voc_v[i], \n",
    "                        0) \n",
    "                      for i in range(len(delta_voc_u))]\n",
    "\n",
    "        # add shear node to the water column with shear list information \n",
    "        if len(shear_list):\n",
    "            water_column.add_shear_node(\n",
    "                z_true=depth,\n",
    "                t=t,\n",
    "                shear_list=shear_list,\n",
    "                voc_ref=voc_ref,\n",
    "                direction=direction,\n",
    "                pitch=pitch,\n",
    "                roll=roll,\n",
    "            )\n",
    "\n",
    "    # add voc_ref measurement to the water column even if shear list is empty  \n",
    "    elif not voc_ref.is_none():\n",
    "        water_column.add_shear_node(\n",
    "            z_true=depth,\n",
    "            t=t,\n",
    "            shear_list=[],\n",
    "            voc_ref=voc_ref,\n",
    "            direction=direction,\n",
    "            pitch=pitch,\n",
    "            roll=roll,\n",
    "        )\n",
    "        \n",
    "voc_u_list,voc_v_list,voc_w_list, z_list = water_column.compute_averages()\n",
    "print(\"> Finished Estimating Water Column Currents!\")\n",
    "#print(water_column.averages_to_str())\n",
    "dvl_plotter.plot_water_column_currents(voc_u_list, voc_v_list, voc_w_list, z_list, save_name='tmp-water-column.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(z_list, voc_u_list)\n",
    "plt.title('u vel Averaged (east-west)')\n",
    "plt.xlabel(\"Depth (m)\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(z_list, voc_v_list)\n",
    "plt.title('v vel Averaged (north-south)')\n",
    "plt.xlabel(\"Depth (m)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='compute-dvl-odometry'></a>\n",
    "## Compute DVL-Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How long (in mins) will algorithm accept ocean current estimates i.e. forgetting factor\n",
    "ocean_current_time_filter = 12.5 # mins\n",
    "MIN_NUM_NODES = 12\n",
    "    \n",
    "# initialize list for new odometry\n",
    "rel_pos_x = [0]\n",
    "rel_pos_y = [0]\n",
    "rel_pos_z = [0]\n",
    "delta_x_list = [0]\n",
    "delta_y_list = [0]\n",
    "\n",
    "\n",
    "\n",
    "vel_list_x = []\n",
    "vel_list_y = []\n",
    "u_list     = []\n",
    "v_list     = []\n",
    "# set flag for setting GPS updates\n",
    "flag_gps_fix_at_surface = False \n",
    "\n",
    "\n",
    "#extract the relevant portion of the glider flight computer\n",
    "start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "dur     = end_t - start_t \n",
    "df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()\n",
    "\n",
    "# extract start_t position \"origin\" from the glider flight data \n",
    "for t in range(len(df_dbd)):\n",
    "    if not np.isnan(df_dbd.m_x_lmc[t]):\n",
    "        dbd_origin_x = df_dbd.m_x_lmc[t]\n",
    "        dbd_origin_y = df_dbd.m_y_lmc[t]\n",
    "        break\n",
    "\n",
    "# iterate through the dive file to update odometry\n",
    "for t in range(1,len(ts.df)):\n",
    "    time    = ts.df.time[t]\n",
    "    prev_x  = rel_pos_x[-1]\n",
    "    prev_y  = rel_pos_y[-1]\n",
    "    delta_t = ts.df.delta_t[t]\n",
    "    depth   = ts.df.depth[t]\n",
    "    \n",
    "    # only use Vtw from pressure sensor when submerged \n",
    "    depth = ts.df.depth[t]\n",
    "    if depth > near_surface_filter:\n",
    "        vtw_u = ts.df.rel_vel_pressure_u[t]\n",
    "        vtw_v = ts.df.rel_vel_pressure_v[t]\n",
    "        flag_gps_fix_at_surface = False\n",
    "    # otherwise use the DVL to estimate the Vtw at the surface\n",
    "    else:\n",
    "        vtw_u = ts.df.rel_vel_dvl_u[t]\n",
    "        vtw_v = ts.df.rel_vel_dvl_v[t]\n",
    "    \n",
    "    # retrieve over ground velocity from DVL in bottom track \n",
    "    vog_u = ts.df.abs_vel_btm_u[t]\n",
    "    vog_v = ts.df.abs_vel_btm_v[t]\n",
    "    #################################################################\n",
    "    # retrieve ocean current estimate from water column \n",
    "    good_node_list = []\n",
    "    count = 0\n",
    "    cum_voc_u = 0\n",
    "    cum_voc_v = 0\n",
    "    # Extract all shear nodes at current depth\n",
    "    wc_depth = water_column.get_wc_bin(depth)\n",
    "    node_list = water_column.get_voc_at_depth(wc_depth)\n",
    "\n",
    "    #Iterate through shear nodes at depth\n",
    "    for shear_node in node_list:\n",
    "        voc = shear_node.voc\n",
    "        if not(voc.is_none()):\n",
    "            # filter out large values when computing averages\n",
    "            if voc.mag < voc_mag_filter:\n",
    "                good_node_list.append(shear_node)\n",
    "    ####################################################################\n",
    "    if (len(good_node_list) > 0):\n",
    "            for i in range(len(good_node_list)):\n",
    "                if i == 0:\n",
    "                    count += 1\n",
    "                    cum_voc_u += good_node_list[0].voc.u\n",
    "                    cum_voc_v += good_node_list[0].voc.v\n",
    "                elif (i <= MIN_NUM_NODES):\n",
    "                    count += 1\n",
    "                    cum_voc_u += good_node_list[i].voc.u\n",
    "                    cum_voc_v += good_node_list[i].voc.v\n",
    "                else: \n",
    "                    time_between_current_estimates = good_node_list[i].t - good_node_list[0].t\n",
    "                    if time_between_current_estimates > (ocean_current_time_filter*60):\n",
    "                        count += 1 \n",
    "                        cum_voc_u += good_node_list[i].voc.u\n",
    "                        cum_voc_v += good_node_list[i].voc.v\n",
    "            #voc_avg = OceanCurrent(cum_voc_u/count, cum_voc_v/count, 0)\n",
    "            voc_u = cum_voc_u/count\n",
    "            voc_v = cum_voc_v/count\n",
    "            u_list.append(voc_u)\n",
    "            v_list.append(voc_v)             \n",
    "    else:\n",
    "        voc_u = np.nan\n",
    "        voc_v = np.nan\n",
    "        u_list.append(voc_u)\n",
    "        v_list.append(voc_v)\n",
    "\n",
    "    #################################################################\n",
    "    # initialize delta values to zero\n",
    "    delta_x, delta_y = 0,0\n",
    "    \n",
    "    # CASE 1: use bottom track overground velocity if available\n",
    "    if (not np.isnan(vog_u)):\n",
    "        delta_x = vog_u*delta_t\n",
    "        delta_y = vog_v*delta_t\n",
    "        vel_list_x.append(vog_u)\n",
    "        vel_list_y.append(vog_v)\n",
    "    \n",
    "    # CASE 2: use through water velocity and ocean current estimate if available\n",
    "    elif (not np.isnan(vtw_u)) and (not np.isnan(voc_u)):\n",
    "            delta_x = (vtw_u + voc_u)*delta_t\n",
    "            delta_y = (vtw_v + voc_v)*delta_t\n",
    "            vel_list_x.append(vtw_u + voc_u)\n",
    "            vel_list_y.append(vtw_v + voc_v)\n",
    "    # CASE 3: use through water velocity if available\n",
    "    elif (not np.isnan(vtw_u)):\n",
    "            delta_x = vtw_u*delta_t\n",
    "            delta_y = vtw_v*delta_t\n",
    "            vel_list_x.append(vtw_u)\n",
    "            vel_list_y.append(vtw_v)\n",
    "    # CASE 4: use ocean current estimate if available\n",
    "    elif (not np.isnan(voc_u)):\n",
    "            delta_x = voc_u*delta_t\n",
    "            delta_y = voc_v*delta_t\n",
    "            vel_list_x.append(voc_u)\n",
    "            vel_list_y.append(voc_v)\n",
    "\n",
    "    # set current position to DVL odometry result \n",
    "    cur_x = delta_x + prev_x\n",
    "    cur_y = delta_y + prev_y\n",
    "    \n",
    "    # override current position if GPS fix is given \n",
    "    if depth < near_surface_filter:\n",
    "        cur_time = datetime.datetime.fromtimestamp(time)\n",
    "        cur_dbd  = df_dbd[str(cur_time):].copy()\n",
    "        if (len(cur_dbd.m_gps_x_lmc) != 0):\n",
    "            if not np.isnan(cur_dbd.m_gps_x_lmc[0]):\n",
    "                cur_x = cur_dbd.m_gps_x_lmc[0] - dbd_origin_x\n",
    "                cur_y = cur_dbd.m_gps_y_lmc[0] - dbd_origin_y\n",
    "                flag_gps_fix_at_surface = True\n",
    "                \n",
    "                vel_list_x.append(cur_dbd.m_vx_lmc[0])\n",
    "                vel_list_y.append(cur_dbd.m_vy_lmc[0])\n",
    "    \n",
    "    # update the odometry list of positions\n",
    "    rel_pos_x.append(cur_x)\n",
    "    rel_pos_y.append(cur_y)\n",
    "    rel_pos_z.append(depth)\n",
    "    delta_x_list.append(delta_x)\n",
    "    delta_y_list.append(delta_y)\n",
    "    \n",
    "    \n",
    "\n",
    "# rel_pos_x_noBL = rel_pos_x\n",
    "# rel_pos_y_noBL = rel_pos_y\n",
    "# add new odomety to the data frame\n",
    "ts.df['rel_pos_x'] = rel_pos_x\n",
    "ts.df['rel_pos_y'] = rel_pos_y\n",
    "ts.df['rel_pos_z'] = rel_pos_z\n",
    "ts.df['delta_x']   = delta_x_list\n",
    "ts.df['delta_y']   = delta_y_list\n",
    "\n",
    "print(\"> Finished Calculating Odometry!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE DVL-ODO without BottomLOCK\n",
    "num_bottom_lock_to_init = 650 #how many iteration will it use before \"turning off\" bottom lock\n",
    "\n",
    "\n",
    "# How long (in mins) will algorithm accept ocean current estimates i.e. forgetting factor\n",
    "ocean_current_time_filter = 12.5 # mins\n",
    "MIN_NUM_NODES = 12\n",
    "    \n",
    "# initialize list for new odometry\n",
    "rel_pos_x = [0]\n",
    "rel_pos_y = [0]\n",
    "rel_pos_z = [0]\n",
    "delta_x_list = [0]\n",
    "delta_y_list = [0]\n",
    "\n",
    "\n",
    "\n",
    "vel_list_x = []\n",
    "vel_list_y = []\n",
    "u_list     = []\n",
    "v_list     = []\n",
    "# set flag for setting GPS updates\n",
    "flag_gps_fix_at_surface = False \n",
    "\n",
    "\n",
    "# #extract the relevant portion of the glider flight computer\n",
    "# start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "# end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "# dur     = end_t - start_t \n",
    "# df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()\n",
    "\n",
    "# # extract start_t position \"origin\" from the glider flight data \n",
    "# for t in range(len(df_dbd)):\n",
    "#     if not np.isnan(df_dbd.m_x_lmc[t]):\n",
    "#         dbd_origin_x = df_dbd.m_x_lmc[t]\n",
    "#         dbd_origin_y = df_dbd.m_y_lmc[t]\n",
    "#         break\n",
    "\n",
    "# iterate through the dive file to update odometry\n",
    "BL_counter = 0\n",
    "for t in range(1,len(ts.df)):\n",
    "    time    = ts.df.time[t]\n",
    "    prev_x  = rel_pos_x[-1]\n",
    "    prev_y  = rel_pos_y[-1]\n",
    "    delta_t = ts.df.delta_t[t]\n",
    "    depth   = ts.df.depth[t]\n",
    "    \n",
    "    # only use Vtw from pressure sensor when submerged \n",
    "    depth = ts.df.depth[t]\n",
    "    if depth > near_surface_filter:\n",
    "        vtw_u = ts.df.rel_vel_pressure_u[t]\n",
    "        vtw_v = ts.df.rel_vel_pressure_v[t]\n",
    "        flag_gps_fix_at_surface = False\n",
    "    # otherwise use the DVL to estimate the Vtw at the surface\n",
    "    else:\n",
    "        vtw_u = ts.df.rel_vel_dvl_u[t]\n",
    "        vtw_v = ts.df.rel_vel_dvl_v[t]\n",
    "    \n",
    "    # retrieve over ground velocity from DVL in bottom track \n",
    "    vog_u = ts.df.abs_vel_btm_u[t]\n",
    "    vog_v = ts.df.abs_vel_btm_v[t]\n",
    "    #################################################################\n",
    "    # retrieve ocean current estimate from water column \n",
    "    good_node_list = []\n",
    "    count = 0\n",
    "    cum_voc_u = 0\n",
    "    cum_voc_v = 0\n",
    "    # Extract all shear nodes at current depth\n",
    "    wc_depth = water_column.get_wc_bin(depth)\n",
    "    node_list = water_column.get_voc_at_depth(wc_depth)\n",
    "\n",
    "    #Iterate through shear nodes at depth\n",
    "    for shear_node in node_list:\n",
    "        voc = shear_node.voc\n",
    "        if not(voc.is_none()):\n",
    "            # filter out large values when computing averages\n",
    "            if voc.mag < voc_mag_filter:\n",
    "                good_node_list.append(shear_node)\n",
    "    ####################################################################\n",
    "    if (len(good_node_list) > 0):\n",
    "            for i in range(len(good_node_list)):\n",
    "                if i == 0:\n",
    "                    count += 1\n",
    "                    cum_voc_u += good_node_list[0].voc.u\n",
    "                    cum_voc_v += good_node_list[0].voc.v\n",
    "                elif (i <= MIN_NUM_NODES):\n",
    "                    count += 1\n",
    "                    cum_voc_u += good_node_list[i].voc.u\n",
    "                    cum_voc_v += good_node_list[i].voc.v\n",
    "                else: \n",
    "                    time_between_current_estimates = good_node_list[i].t - good_node_list[0].t\n",
    "                    if time_between_current_estimates > (ocean_current_time_filter*60):\n",
    "                        count += 1 \n",
    "                        cum_voc_u += good_node_list[i].voc.u\n",
    "                        cum_voc_v += good_node_list[i].voc.v\n",
    "            #voc_avg = OceanCurrent(cum_voc_u/count, cum_voc_v/count, 0)\n",
    "            voc_u = cum_voc_u/count\n",
    "            voc_v = cum_voc_v/count\n",
    "            u_list.append(voc_u)\n",
    "            v_list.append(voc_v)             \n",
    "    else:\n",
    "        voc_u = np.nan\n",
    "        voc_v = np.nan\n",
    "        u_list.append(voc_u)\n",
    "        v_list.append(voc_v)\n",
    "\n",
    "    #################################################################\n",
    "    # initialize delta values to zero\n",
    "    delta_x, delta_y = 0,0\n",
    "    \n",
    "    if BL_counter <= num_bottom_lock_to_init:\n",
    "        BL_counter = BL_counter + 1\n",
    "        # CASE 1: use bottom track overground velocity if available\n",
    "        if (not np.isnan(vog_u)):\n",
    "            delta_x = vog_u*delta_t\n",
    "            delta_y = vog_v*delta_t\n",
    "            vel_list_x.append(vog_u)\n",
    "            vel_list_y.append(vog_v)\n",
    "        # CASE 2: use through water velocity and ocean current estimate if available\n",
    "        elif (not np.isnan(vtw_u)) and (not np.isnan(voc_u)):\n",
    "            delta_x = (vtw_u + voc_u)*delta_t\n",
    "            delta_y = (vtw_v + voc_v)*delta_t\n",
    "            vel_list_x.append(vtw_u + voc_u)\n",
    "            vel_list_y.append(vtw_v + voc_v)\n",
    "        # CASE 3: use through water velocity if available\n",
    "        elif (not np.isnan(vtw_u)):\n",
    "            delta_x = vtw_u*delta_t\n",
    "            delta_y = vtw_v*delta_t\n",
    "            vel_list_x.append(vtw_u)\n",
    "            vel_list_y.append(vtw_v)\n",
    "        # CASE 4: use ocean current estimate if available\n",
    "        elif (not np.isnan(voc_u)):\n",
    "            delta_x = voc_u*delta_t\n",
    "            delta_y = voc_v*delta_t\n",
    "            vel_list_x.append(voc_u)\n",
    "            vel_list_y.append(voc_v)\n",
    "    else:\n",
    "        # CASE 2: use through water velocity and ocean current estimate if available\n",
    "        if (not np.isnan(vtw_u)) and (not np.isnan(voc_u)):\n",
    "                delta_x = (vtw_u + voc_u)*delta_t\n",
    "                delta_y = (vtw_v + voc_v)*delta_t\n",
    "                vel_list_x.append(vtw_u + voc_u)\n",
    "                vel_list_y.append(vtw_v + voc_v)\n",
    "        # CASE 3: use through water velocity if available\n",
    "        elif (not np.isnan(vtw_u)):\n",
    "                delta_x = vtw_u*delta_t\n",
    "                delta_y = vtw_v*delta_t\n",
    "                vel_list_x.append(vtw_u)\n",
    "                vel_list_y.append(vtw_v)\n",
    "        # CASE 4: use ocean current estimate if available\n",
    "        elif (not np.isnan(voc_u)):\n",
    "                delta_x = voc_u*delta_t\n",
    "                delta_y = voc_v*delta_t\n",
    "                vel_list_x.append(voc_u)\n",
    "                vel_list_y.append(voc_v)\n",
    "\n",
    "    # set current position to DVL odometry result \n",
    "    cur_x = delta_x + prev_x\n",
    "    cur_y = delta_y + prev_y\n",
    "    \n",
    "#     # override current position if GPS fix is given \n",
    "#     if depth < near_surface_filter:\n",
    "#         cur_time = datetime.datetime.fromtimestamp(time)\n",
    "#         cur_dbd  = df_dbd[str(cur_time):].copy()\n",
    "#         if (len(cur_dbd.m_gps_x_lmc) != 0):\n",
    "#             if not np.isnan(cur_dbd.m_gps_x_lmc[0]):\n",
    "#                 cur_x = cur_dbd.m_gps_x_lmc[0] - dbd_origin_x\n",
    "#                 cur_y = cur_dbd.m_gps_y_lmc[0] - dbd_origin_y\n",
    "#                 flag_gps_fix_at_surface = True\n",
    "                \n",
    "#                 vel_list_x.append(cur_dbd.m_vx_lmc[0])\n",
    "#                 vel_list_y.append(cur_dbd.m_vy_lmc[0])\n",
    "    \n",
    "    # update the odometry list of positions\n",
    "    rel_pos_x.append(cur_x)\n",
    "    rel_pos_y.append(cur_y)\n",
    "    rel_pos_z.append(depth)\n",
    "    delta_x_list.append(delta_x)\n",
    "    delta_y_list.append(delta_y)\n",
    "    \n",
    "    \n",
    "\n",
    "rel_pos_x_noBL = rel_pos_x\n",
    "rel_pos_y_noBL = rel_pos_y\n",
    "\n",
    "print(\"> Finished Calculating Odometry without bottom lock!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 650\n",
    "plt.figure()\n",
    "plt.plot(ts.df.rel_pos_x, ts.df.rel_pos_y)\n",
    "plt.plot(rel_pos_x_noBL, rel_pos_y_noBL)\n",
    "plt.title('Estimated Track')\n",
    "plt.axis('equal')\n",
    "#plt.axis([-100, 350, -100, 800])\n",
    "plt.xlabel('Eastings [m]')\n",
    "plt.ylabel('Northings [m]')\n",
    "plt.legend(['with bottom-lock', 'no bottom-lock'], loc='lower left', fontsize='xx-small')\n",
    "plt.plot(ts.df.rel_pos_x[test], ts.df.rel_pos_y[test], '+')\n",
    "#plt.plot(x_gps, y_gps)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.df.rel_pos_x, ts.df.rel_pos_y, '*')\n",
    "plt.title('Estimated Track in LMC')\n",
    "plt.xlabel('Eastings [m]')\n",
    "plt.ylabel('Northings [m]')\n",
    "plt.plot(rel_pos_x_noBL, rel_pos_y_noBL, '*')\n",
    "plt.plot(ts.df.rel_pos_x[test], ts.df.rel_pos_y[test], '+')\n",
    "#plt.plot(x_gps, y_gps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ORIGNIAL ACTUAL ALGORITHM BELOW (NOT USED BECAUSE APR10 malfunction with flight computer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# How long (in mins) will algorithm accept ocean current estimates i.e. forgetting factor\n",
    "ocean_current_time_filter = 12.5 # mins\n",
    "MIN_NUM_NODES = 12\n",
    "    \n",
    "# initialize list for new odometry\n",
    "rel_pos_x = [0]\n",
    "rel_pos_y = [0]\n",
    "rel_pos_z = [0]\n",
    "delta_x_list = [0]\n",
    "delta_y_list = [0]\n",
    "\n",
    "\n",
    "\n",
    "vel_list_x = []\n",
    "vel_list_y = []\n",
    "u_list     = []\n",
    "v_list     = []\n",
    "# set flag for setting GPS updates\n",
    "flag_gps_fix_at_surface = False \n",
    "\n",
    "\n",
    "# extract the relevant portion of the glider flight computer\n",
    "start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "dur     = end_t - start_t \n",
    "df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()\n",
    "\n",
    "# extract start_t position \"origin\" from the glider flight data \n",
    "for t in range(len(df_dbd)):\n",
    "    if not np.isnan(df_dbd.m_x_lmc[t]):\n",
    "        dbd_origin_x = df_dbd.m_x_lmc[t]\n",
    "        dbd_origin_y = df_dbd.m_y_lmc[t]\n",
    "        break\n",
    "\n",
    "# iterate through the dive file to update odometry\n",
    "for t in range(1,len(ts.df)):\n",
    "    time    = ts.df.time[t]\n",
    "    prev_x  = rel_pos_x[-1]\n",
    "    prev_y  = rel_pos_y[-1]\n",
    "    delta_t = ts.df.delta_t[t]\n",
    "    depth   = ts.df.depth[t]\n",
    "    \n",
    "    # only use Vtw from pressure sensor when submerged \n",
    "    depth = ts.df.depth[t]\n",
    "    if depth > near_surface_filter:\n",
    "        vtw_u = ts.df.rel_vel_pressure_u[t]\n",
    "        vtw_v = ts.df.rel_vel_pressure_v[t]\n",
    "        flag_gps_fix_at_surface = False\n",
    "    # otherwise use the DVL to estimate the Vtw at the surface\n",
    "    else:\n",
    "        vtw_u = ts.df.rel_vel_dvl_u[t]\n",
    "        vtw_v = ts.df.rel_vel_dvl_v[t]\n",
    "    \n",
    "    # retrieve over ground velocity from DVL in bottom track \n",
    "    vog_u = ts.df.abs_vel_btm_u[t]\n",
    "    vog_v = ts.df.abs_vel_btm_v[t]\n",
    "    #################################################################\n",
    "    # retrieve ocean current estimate from water column \n",
    "    good_node_list = []\n",
    "    count = 0\n",
    "    cum_voc_u = 0\n",
    "    cum_voc_v = 0\n",
    "    # Extract all shear nodes at current depth\n",
    "    wc_depth = water_column.get_wc_bin(depth)\n",
    "    node_list = water_column.get_voc_at_depth(wc_depth)\n",
    "\n",
    "    #Iterate through shear nodes at depth\n",
    "    for shear_node in node_list:\n",
    "        voc = shear_node.voc\n",
    "        if not(voc.is_none()):\n",
    "            # filter out large values when computing averages\n",
    "            if voc.mag < voc_mag_filter:\n",
    "                good_node_list.append(shear_node)\n",
    "    ####################################################################\n",
    "    if (len(good_node_list) > 0):\n",
    "            for i in range(len(good_node_list)):\n",
    "                if i == 0:\n",
    "                    count += 1\n",
    "                    cum_voc_u += good_node_list[0].voc.u\n",
    "                    cum_voc_v += good_node_list[0].voc.v\n",
    "                elif (i <= MIN_NUM_NODES):\n",
    "                    count += 1\n",
    "                    cum_voc_u += good_node_list[i].voc.u\n",
    "                    cum_voc_v += good_node_list[i].voc.v\n",
    "                else: \n",
    "                    time_between_current_estimates = good_node_list[i].t - good_node_list[0].t\n",
    "                    if time_between_current_estimates > (ocean_current_time_filter*60):\n",
    "                        count += 1 \n",
    "                        cum_voc_u += good_node_list[i].voc.u\n",
    "                        cum_voc_v += good_node_list[i].voc.v\n",
    "            #voc_avg = OceanCurrent(cum_voc_u/count, cum_voc_v/count, 0)\n",
    "            voc_u = cum_voc_u/count\n",
    "            voc_v = cum_voc_v/count\n",
    "            u_list.append(voc_u)\n",
    "            v_list.append(voc_v)             \n",
    "    else:\n",
    "        voc_u = np.nan\n",
    "        voc_v = np.nan\n",
    "        u_list.append(voc_u)\n",
    "        v_list.append(voc_v)\n",
    "\n",
    "    #################################################################\n",
    "    # initialize delta values to zero\n",
    "    delta_x, delta_y = 0,0\n",
    "    \n",
    "    # CASE 1: use bottom track overground velocity if available\n",
    "    if (not np.isnan(vog_u)):\n",
    "        delta_x = vog_u*delta_t\n",
    "        delta_y = vog_v*delta_t\n",
    "        vel_list_x.append(vog_u)\n",
    "        vel_list_y.append(vog_v)\n",
    "    \n",
    "    # CASE 2: use through water velocity and ocean current estimate if available\n",
    "    if (not np.isnan(vtw_u)) and (not np.isnan(voc_u)):\n",
    "            delta_x = (vtw_u + voc_u)*delta_t\n",
    "            delta_y = (vtw_v + voc_v)*delta_t\n",
    "            vel_list_x.append(vtw_u + voc_u)\n",
    "            vel_list_y.append(vtw_v + voc_v)\n",
    "    # CASE 3: use through water velocity if available\n",
    "    elif (not np.isnan(vtw_u)):\n",
    "            delta_x = vtw_u*delta_t\n",
    "            delta_y = vtw_v*delta_t\n",
    "            vel_list_x.append(vtw_u)\n",
    "            vel_list_y.append(vtw_v)\n",
    "    # CASE 4: use ocean current estimate if available\n",
    "    elif (not np.isnan(voc_u)):\n",
    "            delta_x = voc_u*delta_t\n",
    "            delta_y = voc_v*delta_t\n",
    "            vel_list_x.append(voc_u)\n",
    "            vel_list_y.append(voc_v)\n",
    "\n",
    "    # set current position to DVL odometry result \n",
    "    cur_x = delta_x + prev_x\n",
    "    cur_y = delta_y + prev_y\n",
    "    \n",
    "    # override current position if GPS fix is given \n",
    "    if depth < near_surface_filter:\n",
    "        cur_time = datetime.datetime.fromtimestamp(time)\n",
    "        cur_dbd  = df_dbd[str(cur_time):].copy()\n",
    "        if (len(cur_dbd.m_gps_x_lmc) != 0):\n",
    "            if not np.isnan(cur_dbd.m_gps_x_lmc[0]):\n",
    "                cur_x = cur_dbd.m_gps_x_lmc[0] - dbd_origin_x\n",
    "                cur_y = cur_dbd.m_gps_y_lmc[0] - dbd_origin_y\n",
    "                flag_gps_fix_at_surface = True\n",
    "                \n",
    "                vel_list_x.append(cur_dbd.m_vx_lmc[0])\n",
    "                vel_list_y.append(cur_dbd.m_vy_lmc[0])\n",
    "    \n",
    "    # update the odometry list of positions\n",
    "    rel_pos_x.append(cur_x)\n",
    "    rel_pos_y.append(cur_y)\n",
    "    rel_pos_z.append(depth)\n",
    "    delta_x_list.append(delta_x)\n",
    "    delta_y_list.append(delta_y)\n",
    "\n",
    "# rel_pos_x_noBL = rel_pos_x\n",
    "# rel_pos_y_noBL = rel_pos_y\n",
    "# add new odomety to the data frame\n",
    "ts.df['rel_pos_x'] = rel_pos_x\n",
    "ts.df['rel_pos_y'] = rel_pos_y\n",
    "ts.df['rel_pos_z'] = rel_pos_z\n",
    "ts.df['delta_x']   = delta_x_list\n",
    "ts.df['delta_y']   = delta_y_list\n",
    "\n",
    "print(\"> Finished Calculating Odometry!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRELIMINARY RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract initial and final GPS fix\n",
    "\n",
    "#Utility Function \n",
    "def get_utm_coords_from_glider_lat_lon(m_lat, m_lon):\n",
    "    SECS_IN_MIN = 60\n",
    "    MIN_OFFSET = 100\n",
    "    lat_min  = m_lat % MIN_OFFSET \n",
    "    lon_min  = m_lon % MIN_OFFSET \n",
    "    lat_dec  = (m_lat - lat_min)/MIN_OFFSET + lat_min/SECS_IN_MIN\n",
    "    lon_dec  = (m_lon - lon_min)/MIN_OFFSET + lon_min/SECS_IN_MIN\n",
    "    utm_pos  = utm.from_latlon(lat_dec, lon_dec)\n",
    "    easting  = round(utm_pos[0],2)\n",
    "    northing = round(utm_pos[1],2)\n",
    "    zone     = utm_pos[2]\n",
    "    zone_letter  = utm_pos[3]\n",
    "    return(easting, northing, zone, zone_letter)\n",
    "\n",
    "#Extract relevant data from flight log (duration of pd0 data)\n",
    "start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "dur     = end_t - start_t \n",
    "df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()\n",
    "\n",
    "# extract start_t position \"origin\" from the glider flight data \n",
    "for t in range(len(df_dbd)):\n",
    "    if not np.isnan(df_dbd.m_x_lmc[t]):\n",
    "        dbd_origin_x = df_dbd.m_x_lmc[t]\n",
    "        dbd_origin_y = df_dbd.m_y_lmc[t]\n",
    "        break\n",
    "\n",
    "# extract start_t position \"origin\" from the glider flight data \n",
    "for t in range(len(df_dbd)):\n",
    "    if not np.isnan(df_dbd.m_x_lmc[t]):\n",
    "        dbd_origin_x_lmc = df_dbd.m_x_lmc[t]\n",
    "        dbd_origin_y_lmc = df_dbd.m_y_lmc[t]\n",
    "        dbd_origin_m_lat = df_dbd.m_lat[t]\n",
    "        dbd_origin_m_lon = df_dbd.m_lon[t]\n",
    "        break\n",
    "\n",
    "dbd_utm_x, dbd_utm_y, _, zone_letter = get_utm_coords_from_glider_lat_lon(\n",
    "    dbd_origin_m_lat, \n",
    "    dbd_origin_m_lon\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "x_gps=df_dbd.m_gps_x_lmc #- dbd_origin_x_lmc, \n",
    "y_gps=df_dbd.m_gps_y_lmc #- dbd_origin_y_lmc,\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.df.rel_pos_x_dvl_dr , ts.df.rel_pos_y_dvl_dr)\n",
    "plt.plot(dbd_origin_x,dbd_origin_y, '+')\n",
    "plt.plot(x_gps,y_gps, '*')\n",
    "plt.title('Estimated Track in LMC')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings [m]')\n",
    "plt.ylabel('Northings [m]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.set(font_scale = 1.5)\n",
    "linewidth = 8\n",
    "#plt_bg = True\n",
    "plt_bg = False\n",
    "\n",
    "sns.scatterplot(\n",
    "    ts.df.rel_pos_x , \n",
    "    ts.df.rel_pos_y , \n",
    "    linewidth=0, \n",
    "    color='limegreen', \n",
    "    label='DVL-Odo',\n",
    "    s=linewidth, \n",
    "    zorder=2,\n",
    ")\n",
    "sns.scatterplot(\n",
    "    rel_pos_x_noBL,\n",
    "    rel_pos_y_noBL,\n",
    "    linewidth=0,\n",
    "    color='mediumorchid', \n",
    "    label='No Bottom-lock',\n",
    "    s=linewidth, \n",
    "    zorder=2,\n",
    ")\n",
    "\n",
    "# sns.scatterplot(\n",
    "#     x=df_dbd.m_x_lmc - dbd_origin_x_lmc,\n",
    "#     y=df_dbd.m_y_lmc - dbd_origin_y_lmc,\n",
    "#     color='mediumorchid',\n",
    "#     label='DR-DACC',\n",
    "#     linewidth=0,\n",
    "#     s=linewidth,\n",
    "#     data=df_dbd,\n",
    "#     zorder=1,\n",
    "# )\n",
    "\n",
    "sns.scatterplot(\n",
    "    ts.df.rel_pos_x[490:491],\n",
    "    ts.df.rel_pos_y[490:491],\n",
    "    marker='X',\n",
    "    color='tab:red', \n",
    "    s=200,\n",
    "    label='GPS Fix',\n",
    "    zorder=7,\n",
    ")\n",
    "# sns.scatterplot(\n",
    "#     df_dbd.m_gps_x_lmc,# + dbd_origin_x, \n",
    "#     df_dbd.m_gps_y_lmc,# + dbd_origin_y,\n",
    "#     marker='X',\n",
    "#     color='tab:red', \n",
    "#     s=200,\n",
    "#     label='GPS Fix',\n",
    "#     zorder=7,\n",
    "# )\n",
    "\n",
    "lgnd = plt.legend(loc='upper left')\n",
    "# for i in range(odos):\n",
    "#     lgnd.legendHandles[i]._sizes = [100]\n",
    "# lgnd.legendHandles[odos]._sizes = [200]\n",
    "\n",
    "plt.axis('equal')\n",
    "xlim=ax.get_xlim()\n",
    "ylim=ax.get_ylim()\n",
    "\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "plt.xlabel('X Position [m]')\n",
    "plt.ylabel('Y Position [m]')\n",
    "#plt.suptitle('DVL-ODO: Dive %s '%dive_label, fontweight='bold')\n",
    "#plt.savefig('/Users/zduguid/Desktop/fig/tmp.png')\n",
    "#plt.close()\n",
    "#print('> Done plotting!')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'time' : ts.df.time ,'x_bl': ts.df.rel_pos_x, 'y_bl': ts.df.rel_pos_y, 'x_no_bl': rel_pos_x_noBL, 'y_no_bl': rel_pos_y_noBL, 'depth': ts.df.depth }\n",
    "df = pd.DataFrame(data=d)\n",
    "df.to_csv('OCEANS_GREG_DATA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ts.df.rel_pos_x_dvl_dr, ts.df.rel_pos_y_dvl_dr)\n",
    "plt.title('Estimated Track in LMC')\n",
    "plt.axis('equal')\n",
    "plt.xlabel('Eastings [m]')\n",
    "plt.ylabel('Northings [m]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(-1*ts.df.depth[488:-80], 'o') # 490 is last GPS fix\n",
    "plt.title('Depth')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('Depth [m]')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.df.time,ts.df.pitch)\n",
    "plt.title('Pitch [deg]')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('Pitch [deg]')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.df.time,ts.df.roll)\n",
    "plt.title('Roll [deg]')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('Roll [deg]')\n",
    "\n",
    "#wrapping problem\n",
    "plt.figure()\n",
    "heading = []\n",
    "for val in ts.df.heading:\n",
    "    if val > 180 :\n",
    "        heading.append(val-360)\n",
    "    else:\n",
    "        heading.append(val)        \n",
    "plt.plot(ts.df.time,heading)\n",
    "plt.title('Heading [deg]')\n",
    "plt.xlabel('time [s]')\n",
    "plt.ylabel('Heading [deg]')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.df.rel_pos_x, ts.df.rel_pos_y)\n",
    "plt.plot(rel_pos_x_noBL, rel_pos_y_noBL)\n",
    "plt.title('Estimated Track')\n",
    "plt.axis('equal')\n",
    "#plt.axis([-100, 350, -100, 800])\n",
    "plt.xlabel('Eastings [m]')\n",
    "plt.ylabel('Northings [m]')\n",
    "plt.legend(['with bottom-lock', 'no bottom-lock'], loc='lower right')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(ts.df.rel_pos_x, ts.df.rel_pos_y, '*')\n",
    "plt.title('Estimated Track in LMC')\n",
    "plt.xlabel('Eastings [m]')\n",
    "plt.ylabel('Northings [m]')\n",
    "plt.plot(rel_pos_x_noBL, rel_pos_y_noBL, '*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO everywhere where depth is 1 m\n",
    "start = 490\n",
    "sum(abs(ts.df.pitch[start:-100]))/len(ts.df.pitch[start:-100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(df_dbd.m_pitch)\n",
    "#df_dbd.to_csv('APR10_dbd_fromSENTLOG_mainDive.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(u_list)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(v_list)\n",
    "len(v_list)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(v_list,-ts.df.depth[0:-1], 'o')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(u_list, -ts.df.depth[0:-1],  'o')\n",
    "\n",
    "u_list = np.array(u_list)\n",
    "v_list = np.array(v_list)\n",
    "\n",
    "magnitude = np.sqrt((v_list**2) + (u_list**2))\n",
    "plt.figure()\n",
    "plt.plot(magnitude, -ts.df.depth[0:-1])\n",
    "\n",
    "#TODO re-bin at 3m for useful plot to help recognize (or not) accuracy --> trade off resolution for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(ts.df.time, ts.df.abs_vel_btm_u)\n",
    "plt.figure()\n",
    "plt.plot(ts.df.time,ts.df.abs_vel_btm_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='multi-factor-terrain-based-navigation'></a>\n",
    "## Multi-Factor Terrain Based Navigation (MF-TAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload_modules()\n",
    "\n",
    "def get_utm_coords_from_glider_lat_lon(m_lat, m_lon):\n",
    "    SECS_IN_MIN = 60\n",
    "    MIN_OFFSET = 100\n",
    "    lat_min  = m_lat % MIN_OFFSET \n",
    "    lon_min  = m_lon % MIN_OFFSET \n",
    "    lat_dec  = (m_lat - lat_min)/MIN_OFFSET + lat_min/SECS_IN_MIN\n",
    "    lon_dec  = (m_lon - lon_min)/MIN_OFFSET + lon_min/SECS_IN_MIN\n",
    "    utm_pos  = utm.from_latlon(lat_dec, lon_dec)\n",
    "    easting  = round(utm_pos[0],2)\n",
    "    northing = round(utm_pos[1],2)\n",
    "    zone     = utm_pos[2]\n",
    "    zone_letter  = utm_pos[3]\n",
    "    return(easting, northing, zone, zone_letter)\n",
    "\n",
    "# constants\n",
    "JANUS_ANGLE = 30\n",
    "DEG_TO_RAD  = np.pi/180\n",
    "RAD_TO_DEG  = 1/DEG_TO_RAD\n",
    "sin_janus   = np.sin(JANUS_ANGLE*DEG_TO_RAD)\n",
    "cos_janus   = np.cos(JANUS_ANGLE*DEG_TO_RAD)\n",
    "min_valid_slant_ranges = 3 \n",
    "\n",
    "# AUG parameters\n",
    "BIAS_PITCH   = 12.5  # [deg]\n",
    "BIAS_ROLL    =  0.0  # [deg]\n",
    "BIAS_HEADING =  0.0  # [deg]\n",
    "\n",
    "\n",
    "# # TAN parameters\n",
    "############################\n",
    "# Long Dive (Dive A) ########\n",
    "# DVL_ODO_DRIFT = 0.15 #Heuristic that determines growth of uncertaintyh based on worst-case DVL-ODO performance history\n",
    "DVL_ODO_DRIFT = 0.60\n",
    "TAN_RED_DRIFT = 0.70 #Every time there is a a valid MF-TAN fix, reduces r (positional uncertainty) by 30%\n",
    "# TAU_DEPTH     = 2\n",
    "# TAU_SLOPE     = 2\n",
    "# TAU_ORIENT    = 4\n",
    "# MIN_PITCH_FOR_ORIENT = 10\n",
    "# TAN_WEIGHT    = 0.4\n",
    "# TAU_DEPTH     = 5\n",
    "# TAU_SLOPE     = 15\n",
    "# TAU_ORIENT    = 10\n",
    "# MIN_PITCH_FOR_ORIENT = 10\n",
    "# TAN_WEIGHT    = 0.4\n",
    "TAN_WEIGHT    = 0.5\n",
    "\n",
    "# ###########################\n",
    "# # Short Dive (Dive F) #######\n",
    "# DVL_ODO_DRIFT = 0.20\n",
    "# TAN_RED_DRIFT = 0.90\n",
    "# TAU_DEPTH     = 1\n",
    "# TAU_SLOPE     = 20\n",
    "# TAU_ORIENT    = 30\n",
    "# MIN_PITCH_FOR_ORIENT = 10\n",
    "# TAN_WEIGHT    = 0.4\n",
    "\n",
    "#############################\n",
    "# TEMPORARY #################\n",
    "# DVL_ODO_DRIFT = 0.20\n",
    "# TAN_RED_DRIFT = 0.90\n",
    "# TAU_DEPTH     = 1\n",
    "# TAU_SLOPE     = 0\n",
    "# TAU_ORIENT    = 0\n",
    "# MIN_PITCH_FOR_ORIENT = 10\n",
    "# TAN_WEIGHT    = 0.4\n",
    "\n",
    "################# TEMPORARY --> NEED to UNDERSTAND FULL EFFECT\n",
    "# factor_depth_point  = 288.39 \n",
    "# factor_slope_point  = 30.08\n",
    "# factor_orient_point = 129.23\n",
    "\n",
    "\n",
    "# heading offsets for the four DVL beams\n",
    "beam_heading_offsets = {\n",
    "    0 : -90, # 0 = Port\n",
    "    1 :  90, # 1 = Starboard\n",
    "    2 :   0, # 2 = Forward\n",
    "    3 : 180, # 3 = Aft\n",
    "}\n",
    "\n",
    "            \n",
    "# intialize point cloud object \n",
    "pc = MultiFactorTAN.PointCloud(grid_res_num)\n",
    "pc_bathy_depth  = [np.nan]\n",
    "pc_bathy_slope  = [np.nan]\n",
    "pc_bathy_orient = [np.nan]\n",
    "MFTAN_depth  = np.array(bathy_df.depth_list)\n",
    "MFTAN_slope  = np.array(bathy_df.slope_list)\n",
    "MFTAN_orient = np.array(bathy_df.orient_list)\n",
    "\n",
    "\n",
    "# initialize list to keep track of TAN information\n",
    "tan_pos_x = [0]\n",
    "tan_pos_y = [0]\n",
    "tan_pos_z = [0]\n",
    "tan_pos_r = [0]\n",
    "dvl_pos_r = [0]\n",
    "sf_tan_pos_x = [0]\n",
    "sf_tan_pos_y = [0]\n",
    "tan_update_x = []\n",
    "tan_update_y = []\n",
    "tan_update_t = []\n",
    "tan_update_index  = [] \n",
    "tan_update_depth  = []\n",
    "tan_update_slope  = []\n",
    "tan_update_orient = []\n",
    "\n",
    "tan_update_x_SF = []\n",
    "tan_update_y_SF = []\n",
    "tan_update_t_SF = []\n",
    "tan_update_index_SF  = [] \n",
    "\n",
    "\n",
    "# extract the relevant portion of the glider flight computer\n",
    "start_t = datetime.datetime.fromtimestamp(ts.df.time[0])\n",
    "end_t   = datetime.datetime.fromtimestamp(ts.df.time[-1])\n",
    "dur     = end_t - start_t \n",
    "df_dbd  = ts_flight_kolumbo_all.df[str(start_t):str(end_t)].copy()\n",
    "\n",
    "# extract start_t position \"origin\" from the glider flight data \n",
    "for t in range(len(df_dbd)):\n",
    "    if not np.isnan(df_dbd.m_x_lmc[t]):\n",
    "        dbd_origin_x_lmc = df_dbd.m_x_lmc[t]\n",
    "        dbd_origin_y_lmc = df_dbd.m_y_lmc[t]\n",
    "        dbd_origin_m_lat = df_dbd.m_lat[t]\n",
    "        dbd_origin_m_lon = df_dbd.m_lon[t]\n",
    "        break\n",
    "\n",
    "dbd_utm_x, dbd_utm_y, _, zone_letter = get_utm_coords_from_glider_lat_lon(\n",
    "    dbd_origin_m_lat, \n",
    "    dbd_origin_m_lon\n",
    ")\n",
    "\n",
    "##Adaptive TAU Testing Logs##\n",
    "depth_var_list  = []\n",
    "slope_var_list  = []\n",
    "orient_var_list = []\n",
    "utm_est_x_list  = []\n",
    "utm_est_y_list  = []\n",
    "#############################\n",
    "\n",
    "# iterate over length of Dive \n",
    "for t in range(1,len(ts.df)):\n",
    "        \n",
    "    # retrieve previous position information\n",
    "    time    = ts.df.time[t]\n",
    "    prev_x  = tan_pos_x[-1]\n",
    "    prev_y  = tan_pos_y[-1]\n",
    "    prev_r  = tan_pos_r[-1]\n",
    "    delta_t = ts.df.delta_t[t]\n",
    "    depth   = ts.df.depth[t]\n",
    "    delta_x = ts.df.delta_x[t]\n",
    "    delta_y = ts.df.delta_y[t]\n",
    "    delta_r = np.linalg.norm([delta_x, delta_y])\n",
    "    sf_prev_x  = sf_tan_pos_x[-1]\n",
    "    sf_prev_y  = sf_tan_pos_y[-1]\n",
    "    \n",
    "    # retrieve DVL odometry update for case when TAN fix not available\n",
    "    dvl_odo_x = prev_x + delta_x\n",
    "    dvl_odo_y = prev_y + delta_y\n",
    "    sf_dvl_odo_x = sf_prev_x + delta_x\n",
    "    sf_dvl_odo_y = sf_prev_y + delta_y\n",
    "    dvl_odo_r = prev_r + delta_r*DVL_ODO_DRIFT\n",
    "    dvl_pos_r.append(dvl_pos_r[-1]+delta_r*DVL_ODO_DRIFT)\n",
    "    \n",
    "    # extract slant ranges \n",
    "    slant_ranges = {\n",
    "        0 : ts.df.btm_beam0_range[t] / cos_janus, # 0 = Port \n",
    "        1 : ts.df.btm_beam1_range[t] / cos_janus, # 1 = Starboard\n",
    "        2 : ts.df.btm_beam2_range[t] / cos_janus, # 2 = Forward  \n",
    "        3 : ts.df.btm_beam3_range[t] / cos_janus, # 3 = Aft \n",
    "    }\n",
    "\n",
    "    # ignore case when less than three ranges are available\n",
    "    valid_slant_ranges = {key:slant_ranges[key] for key in \n",
    "        slant_ranges.keys() if not np.isnan(slant_ranges[key])}\n",
    "        \n",
    "    # extract current AUV position in LMC coordinates\n",
    "    aug_x = ts.df.rel_pos_x[t]\n",
    "    aug_y = ts.df.rel_pos_y[t]\n",
    "    aug_z = ts.df.rel_pos_z[t]\n",
    "    aug_heading = ts.df.heading[t]\n",
    "    aug_pitch   = ts.df.pitch[t]\n",
    "    aug_roll    = ts.df.roll[t]\n",
    "    \n",
    "    # override current position if GPS fix is given \n",
    "    if depth < near_surface_filter:\n",
    "        cur_time = datetime.datetime.fromtimestamp(time)\n",
    "        cur_dbd  = df_dbd[str(cur_time):].copy()\n",
    "        if (len(cur_dbd.m_gps_x_lmc) != 0):\n",
    "            if not np.isnan(cur_dbd.m_gps_x_lmc[0]):\n",
    "                gps_x = cur_dbd.m_gps_x_lmc[0] - dbd_origin_x\n",
    "                gps_y = cur_dbd.m_gps_y_lmc[0] - dbd_origin_y\n",
    "                flag_gps_fix_at_surface = True\n",
    "                pc_bathy_depth.append(np.nan)\n",
    "                pc_bathy_slope.append(np.nan)\n",
    "                pc_bathy_orient.append(np.nan)\n",
    "                tan_pos_x.append(gps_x)\n",
    "                tan_pos_y.append(gps_y)\n",
    "                tan_pos_z.append(depth)\n",
    "                sf_tan_pos_x.append(gps_x)\n",
    "                sf_tan_pos_y.append(gps_y)\n",
    "                new_r = np.min([prev_r*0.5, 50])\n",
    "                tan_pos_r.append(prev_r)\n",
    "                continue\n",
    "    \n",
    "    # ignore case when 3 or less slant ranges are present\n",
    "    # ignore case when glider is not sufficiently pitched\n",
    "    #######################################Pose Filter##################### REMOVED\n",
    "    if (len(valid_slant_ranges) < min_valid_slant_ranges): #or (abs(aug_pitch) < pc.MIN_PITCH)):\n",
    "        pc_bathy_depth.append(np.nan)\n",
    "        pc_bathy_slope.append(np.nan)\n",
    "        pc_bathy_orient.append(np.nan)\n",
    "        tan_pos_x.append(dvl_odo_x)\n",
    "        tan_pos_y.append(dvl_odo_y)\n",
    "        tan_pos_z.append(depth)\n",
    "        tan_pos_r.append(dvl_odo_r)\n",
    "        sf_tan_pos_x.append(sf_dvl_odo_x)\n",
    "        sf_tan_pos_y.append(sf_dvl_odo_y)\n",
    "        continue\n",
    "    \n",
    "    # compute rotation matrices to go from instrument coords to earth coords\n",
    "    aug_Qx = pc.Qx((aug_pitch   + BIAS_PITCH)   * DEG_TO_RAD)\n",
    "    aug_Qy = pc.Qy((aug_roll    + BIAS_ROLL)    * DEG_TO_RAD)\n",
    "    aug_Qz = pc.Qz((aug_heading + BIAS_HEADING) * DEG_TO_RAD)\n",
    "\n",
    "    # extract bottom contact positions in Earth coordinate frame\n",
    "    point_cloud = []\n",
    "    for beam in valid_slant_ranges:\n",
    "        r = valid_slant_ranges[beam]\n",
    "        z = r*cos_janus  # vertical component \n",
    "        h = r*sin_janus  # horizontal component\n",
    "        \n",
    "        # get bottom contact in instrument coordinates\n",
    "        beam_heading = beam_heading_offsets[beam]\n",
    "        x  = h*np.sin(beam_heading*DEG_TO_RAD)\n",
    "        y  = h*np.cos(beam_heading*DEG_TO_RAD)\n",
    "        z *= -1  # z is positive upwards for rotation\n",
    "        \n",
    "        # rotate into Ship coordinates\n",
    "        # + ship coordinates is a horizontal translation away from Earth coordinates\n",
    "        inst_pos = np.array([[x], [y], [z]])\n",
    "        ship_pos = np.dot(aug_Qz, np.dot(aug_Qy, np.dot(aug_Qx, inst_pos)))\n",
    "        x,y,z    = tuple(ship_pos.flatten())\n",
    "        z       *= -1  # z is positive downwards again\n",
    "        \n",
    "        # add to the point cloud\n",
    "        # + keep track of ship coordinates for debugging purposes\n",
    "        bt_point = MultiFactorTAN.BottomTrackPoint(t, beam, x, y, z, aug_x, aug_y, aug_z)\n",
    "        pc.add_point(bt_point)\n",
    "    \n",
    "    # get the three bathymetry factors from the point cloud\n",
    "    bathy_depth, bathy_slope, bathy_orient = pc.get_factors()\n",
    "    pc_bathy_depth.append(bathy_depth)\n",
    "    pc_bathy_slope.append(bathy_slope)\n",
    "    pc_bathy_orient.append(bathy_orient)\n",
    "    \n",
    "    # update use DVL-Odometry update when no features are available\n",
    "    # + navigation uncertainty r grows as a function of distance traveled\n",
    "    if np.isnan(bathy_depth):\n",
    "        tan_pos_x.append(dvl_odo_x)\n",
    "        tan_pos_y.append(dvl_odo_y)\n",
    "        tan_pos_z.append(depth)\n",
    "        tan_pos_r.append(dvl_odo_r)\n",
    "        sf_tan_pos_x.append(sf_dvl_odo_x)\n",
    "        sf_tan_pos_y.append(sf_dvl_odo_y)\n",
    "        continue\n",
    "    \n",
    "    ###########################################\n",
    "    #       Adaptive Tau Process *TESTING*    #\n",
    "    ###########################################\n",
    "    def find_nearest(array1, value1, array2, value2):\n",
    "        array1 = np.asarray(array1)\n",
    "        array2 = np.asarray(array2)\n",
    "        idx = (np.abs(array1 - value1) + np.abs(array2-value2)).argmin()\n",
    "        return idx\n",
    "    \n",
    "    def findVar_fromMap(x, y, df):\n",
    "        \n",
    "        idx = find_nearest(df.utm_x_list, x, df.utm_y_list, y)\n",
    "        \n",
    "        depth_var  = df.depth_var[idx]\n",
    "        slope_var  = df.slope_var[idx]\n",
    "        orient_var = df.orient_var[idx]\n",
    "        return depth_var, slope_var, orient_var\n",
    "    \n",
    "    def generateTAU(depth_var, slope_var, orient_var):\n",
    "        # How do we model adding the noise/uncertainty/error that comes from the sensor and our factor extraction process\n",
    "        # This shouldn't change and should be independent of the variability of the ocean floor\n",
    "        DEPTH_NOISE_FACTOR =  0 # [m]\n",
    "        SLOPE_NOISE_FACTOR =  2.5 # [deg]\n",
    "        ORIENT_NOISE_FACTOR = 12 # [deg]\n",
    "        TAU_DEPTH  = np.sqrt(depth_var)*3 + DEPTH_NOISE_FACTOR\n",
    "        TAU_SLOPE  = np.sqrt(slope_var)*3 + SLOPE_NOISE_FACTOR\n",
    "        TAU_ORIENT = np.sqrt(orient_var)*0.5 + ORIENT_NOISE_FACTOR\n",
    "\n",
    "        \n",
    "        return TAU_DEPTH, TAU_SLOPE, TAU_ORIENT\n",
    "    \n",
    "    cur_utm_est_x = dbd_utm_x + dvl_odo_x\n",
    "    cur_utm_est_y = dbd_utm_y + dvl_odo_y\n",
    "    \n",
    "    cur_depth_var, cur_slope_var, cur_orient_var = findVar_fromMap(cur_utm_est_x, cur_utm_est_y, bathy_df_var)\n",
    "    \n",
    "\n",
    "    TAU_DEPTH, TAU_SLOPE, TAU_ORIENT = generateTAU(cur_depth_var, cur_slope_var, cur_orient_var)\n",
    "    \n",
    "    depth_var_list.append(TAU_DEPTH)\n",
    "    slope_var_list.append(TAU_SLOPE)\n",
    "    orient_var_list.append(TAU_ORIENT)\n",
    "    utm_est_x_list.append(cur_utm_est_x)\n",
    "    utm_est_y_list.append(cur_utm_est_y)\n",
    "    ############################################\n",
    "    \n",
    "    # use factors to help limit navigation error \n",
    "    MFTAN_factors = np.array(bathy_df.depth_list)\n",
    "    MFTAN_factors[MFTAN_depth > bathy_depth+TAU_DEPTH] = np.nan\n",
    "    MFTAN_factors[MFTAN_depth < bathy_depth-TAU_DEPTH] = np.nan\n",
    "    MFTAN_factors[MFTAN_slope > bathy_slope+TAU_SLOPE] = np.nan\n",
    "    MFTAN_factors[MFTAN_slope < bathy_slope-TAU_SLOPE] = np.nan\n",
    "    \n",
    "    MFTAN_factors[MFTAN_orient > bathy_orient+TAU_ORIENT] = np.nan\n",
    "    MFTAN_factors[MFTAN_orient < bathy_orient-TAU_ORIENT] = np.nan\n",
    "    \n",
    "#     # dont use orientation factor for low pitch \n",
    "#     if bathy_slope > MIN_PITCH_FOR_ORIENT:            \n",
    "#         lowerbound = factor_orient_point - TAU_ORIENT\n",
    "#         upperbound = factor_orient_point + TAU_ORIENT\n",
    "#         if upperbound > 180:\n",
    "#             upperbound -= 360\n",
    "#             MFTAN_factors[(MFTAN_orient > upperbound)] = np.nan\n",
    "#             MFTAN_factors[(MFTAN_orient < lowerbound)] = np.nan\n",
    "#             # MFTAN_factors[((MFTAN_orient > upperbound) & (MFTAN_orient1 <= 0))] = np.nan\n",
    "#             # MFTAN_factors[((MFTAN_orient < lowerbound) & (MFTAN_orient1 >= 0))] = np.nan\n",
    "#         elif lowerbound < -180:\n",
    "#             lowerbound += 360\n",
    "#             MFTAN_factors[(MFTAN_orient > upperbound)] = np.nan\n",
    "#             MFTAN_factors[(MFTAN_orient < lowerbound)] = np.nan\n",
    "#             # MFTAN_factors[((MFTAN_orient > upperbound) & (MFTAN_orient1 <= 0))] = np.nan\n",
    "#             # MFTAN_factors[((MFTAN_orient < lowerbound) & (MFTAN_orient1 >= 0))] = np.nan\n",
    "#         else:\n",
    "#             MFTAN_factors[MFTAN_orient < lowerbound] = np.nan\n",
    "            \n",
    "    # Single-Factor TAN equivalent\n",
    "    SFTAN_factors = np.array(bathy_df.depth_list)\n",
    "    SFTAN_factors[MFTAN_depth > bathy_depth+TAU_DEPTH] = np.nan\n",
    "    SFTAN_factors[MFTAN_depth < bathy_depth-TAU_DEPTH] = np.nan\n",
    "        \n",
    "    MFTAN_factors[((bathy_df.utm_x_list - dbd_utm_x - prev_x)**2 + \n",
    "                   (bathy_df.utm_y_list - dbd_utm_y - prev_y)**2)**0.5 > prev_r] = np.nan\n",
    "    SFTAN_factors[((bathy_df.utm_x_list - dbd_utm_x - prev_x)**2 + \n",
    "                   (bathy_df.utm_y_list - dbd_utm_y - prev_y)**2)**0.5 > prev_r] = np.nan\n",
    "    \n",
    "    MFTAN_factors = np.array(MFTAN_factors)\n",
    "    SFTAN_factors = np.array(SFTAN_factors)\n",
    "    idx           = np.argwhere(np.isfinite(MFTAN_factors)).flatten()\n",
    "    SF_idx        = np.argwhere(np.isfinite(SFTAN_factors)).flatten()\n",
    "    \n",
    "    # if match found, update pos and reduce uncertainty \n",
    "    # + possibly expand uncertainty range to sett if fix is available?\n",
    "    if len(idx) > 0:\n",
    "        MFTAN_x = np.mean([bathy_df.utm_x_list[_] for _ in idx]) - dbd_utm_x\n",
    "        MFTAN_y = np.mean([bathy_df.utm_y_list[_] for _ in idx]) - dbd_utm_y\n",
    "         \n",
    "        ODO_WEIGHT = 1-TAN_WEIGHT\n",
    "        new_x   = ODO_WEIGHT*dvl_odo_x + TAN_WEIGHT*MFTAN_x\n",
    "        new_y   = ODO_WEIGHT*dvl_odo_y + TAN_WEIGHT*MFTAN_y\n",
    "        tan_pos_x.append(new_x)\n",
    "        tan_pos_y.append(new_y)\n",
    "        tan_pos_z.append(depth)\n",
    "        tan_pos_r.append(prev_r*TAN_RED_DRIFT)\n",
    "        \n",
    "        # store TAN fix information for plotting utilities\n",
    "        tan_update_x.append(new_x)\n",
    "        tan_update_y.append(new_y)\n",
    "        tan_update_t.append(ts.df.time[t])\n",
    "        tan_update_index.append(t)\n",
    "        tan_update_depth.append(bathy_depth)\n",
    "        tan_update_slope.append(bathy_slope)\n",
    "        tan_update_orient.append(bathy_orient)\n",
    "        \n",
    "        \n",
    "    # not matches with MF-TAN, use SF-TAN or DVL-Odometry if necessary\n",
    "    else:\n",
    "        if len(SF_idx) > 0:\n",
    "            MFTAN_x = np.mean([bathy_df.utm_x_list[_] for _ in SF_idx]) - dbd_utm_x\n",
    "            MFTAN_y = np.mean([bathy_df.utm_y_list[_] for _ in SF_idx]) - dbd_utm_y\n",
    "            ODO_WEIGHT = 1-TAN_WEIGHT\n",
    "            new_x   = ODO_WEIGHT*dvl_odo_x + TAN_WEIGHT*MFTAN_x\n",
    "            new_y   = ODO_WEIGHT*dvl_odo_y + TAN_WEIGHT*MFTAN_y\n",
    "            \n",
    "            tan_pos_x.append(new_x)\n",
    "            tan_pos_y.append(new_y)\n",
    "            tan_pos_z.append(depth)\n",
    "            tan_pos_r.append(prev_r*TAN_RED_DRIFT)\n",
    "            \n",
    "            # store TAN fix information for plotting utilities\n",
    "            tan_update_x_SF.append(new_x)\n",
    "            tan_update_y_SF.append(new_y)\n",
    "            tan_update_t_SF.append(ts.df.time[t])\n",
    "            tan_update_index_SF.append(t)\n",
    "            tan_update_depth.append(bathy_depth)\n",
    "            tan_update_slope.append(bathy_slope)\n",
    "            tan_update_orient.append(bathy_orient)\n",
    "        \n",
    "        # otherwise use DVL \n",
    "        else:\n",
    "            tan_pos_x.append(dvl_odo_x)\n",
    "            tan_pos_y.append(dvl_odo_y)\n",
    "            tan_pos_z.append(depth)\n",
    "            tan_pos_r.append(dvl_odo_r)\n",
    "        \n",
    "\n",
    "    # if match found, update pos and reduce uncertainty \n",
    "    # + possibly expand uncertainty range to sett if fix is available?\n",
    "    if len(SF_idx) > 0:\n",
    "        SFTAN_x = np.mean([bathy_df.utm_x_list[_] for _ in SF_idx]) - dbd_utm_x\n",
    "        SFTAN_y = np.mean([bathy_df.utm_y_list[_] for _ in SF_idx]) - dbd_utm_y\n",
    "        ODO_WEIGHT = 1-TAN_WEIGHT\n",
    "        new_x   = ODO_WEIGHT*sf_dvl_odo_x + TAN_WEIGHT*SFTAN_x\n",
    "        new_y   = ODO_WEIGHT*sf_dvl_odo_y + TAN_WEIGHT*SFTAN_y\n",
    "        sf_tan_pos_x.append(new_x)\n",
    "        sf_tan_pos_y.append(new_y)\n",
    "        \n",
    "    # not matches with MF-TAN -- update using DVL-odometry\n",
    "    else:\n",
    "        sf_tan_pos_x.append(sf_dvl_odo_x)\n",
    "        sf_tan_pos_y.append(sf_dvl_odo_y)\n",
    "    \n",
    "    \n",
    "# add seafloor factors to the dataframe\n",
    "pc_bathy_depth  = np.array(pc_bathy_depth)\n",
    "pc_bathy_slope  = np.array(pc_bathy_slope)\n",
    "pc_bathy_orient = np.array(pc_bathy_orient)\n",
    "ts.df.pc_bathy_depth  = pc_bathy_depth\n",
    "ts.df.pc_bathy_slope  = pc_bathy_slope\n",
    "ts.df.pc_bathy_orient = pc_bathy_orient\n",
    "\n",
    "# add new odomety to the data frame\n",
    "ts.df['tan_pos_x'] = tan_pos_x\n",
    "ts.df['tan_pos_y'] = tan_pos_y\n",
    "ts.df['tan_pos_z'] = tan_pos_z\n",
    "ts.df['tan_pos_r'] = tan_pos_r\n",
    "\n",
    "print(\"> Finished Multi-Factor Terrain-Aided Navigation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='plot-navigation-results'></a>\n",
    "### A. Plot Navigation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.set(font_scale = 1.5)\n",
    "linewidth = 8\n",
    "#plt_bg = True\n",
    "plt_bg = False\n",
    "\n",
    "'''\n",
    "sns.scatterplot(\n",
    "    tan_pos_x, \n",
    "    tan_pos_y, \n",
    "    linewidth=0, \n",
    "    color='tab:orange', \n",
    "    label='MF-TAN',\n",
    "    s=linewidth, \n",
    "    zorder=4,\n",
    ")\n",
    "odos=3\n",
    "\n",
    "sns.scatterplot(\n",
    "    sf_tan_pos_x, \n",
    "    sf_tan_pos_y, \n",
    "    linewidth=0, \n",
    "    color='black', \n",
    "    label='SF-TAN',\n",
    "    s=10, \n",
    "    zorder=3,\n",
    ")\n",
    "odos=4\n",
    "'''\n",
    "sns.scatterplot(\n",
    "    ts.df.rel_pos_x, \n",
    "    #rel_x,\n",
    "    #rel_y,\n",
    "    ts.df.rel_pos_y, \n",
    "    linewidth=0, \n",
    "    color='limegreen', \n",
    "    label='DVL-Odo',\n",
    "    s=linewidth, \n",
    "    zorder=2,\n",
    ")\n",
    "sns.scatterplot(\n",
    "    #ts.df.rel_pos_x, \n",
    "    rel_pos_x_noBL,\n",
    "    rel_pos_y_noBL,\n",
    "    #ts.df.rel_pos_y, \n",
    "    linewidth=0, \n",
    "    color='mediumorchid', \n",
    "    label='NO-BL',\n",
    "    s=linewidth, \n",
    "    zorder=2,\n",
    ")\n",
    "\n",
    "# sns.scatterplot(\n",
    "#     x=df_dbd.m_x_lmc - dbd_origin_x_lmc,\n",
    "#     y=df_dbd.m_y_lmc - dbd_origin_y_lmc,\n",
    "#     color='mediumorchid',\n",
    "#     label='DR-DACC',\n",
    "#     linewidth=0,\n",
    "#     s=linewidth,\n",
    "#     data=df_dbd,\n",
    "#     zorder=1,\n",
    "# )\n",
    "\n",
    "#             tan_update_log.append(len(tan_update_x))\n",
    "#             sns.scatterplot(\n",
    "#                 tan_update_x, \n",
    "#                 tan_update_y, \n",
    "#                 zorder=4, \n",
    "#                 marker='^', \n",
    "#                 label='TAN Fix',\n",
    "#                 s=60,\n",
    "#             )\n",
    "'''\n",
    "sns.scatterplot(\n",
    "    tan_update_x, \n",
    "    tan_update_y, \n",
    "    zorder=5, \n",
    "    marker='^', \n",
    "    label='MF-TAN Fix',\n",
    "    s=60,\n",
    ")\n",
    "\n",
    "# tan_update_log.append(len(tan_update_x))\n",
    "# tan_update_log_SF.append(len(tan_update_x_SF))\n",
    "sns.scatterplot(\n",
    "    tan_update_x_SF, \n",
    "    tan_update_y_SF, \n",
    "    zorder=6, \n",
    "    marker='^',\n",
    "    label='SF-TAN Fix',\n",
    "    s=80,\n",
    ")\n",
    "'''\n",
    "sns.scatterplot(\n",
    "    x=df_dbd.m_gps_x_lmc - dbd_origin_x_lmc, \n",
    "    y=df_dbd.m_gps_y_lmc - dbd_origin_y_lmc,\n",
    "    marker='X',\n",
    "    color='tab:red', \n",
    "    s=200,\n",
    "    label='GPS Fix',\n",
    "    data=df_dbd,\n",
    "    zorder=7,\n",
    ")\n",
    "\n",
    "lgnd = plt.legend(loc='upper left')\n",
    "# for i in range(odos):\n",
    "#     lgnd.legendHandles[i]._sizes = [100]\n",
    "# lgnd.legendHandles[odos]._sizes = [200]\n",
    "\n",
    "plt.axis('equal')\n",
    "xlim=ax.get_xlim()\n",
    "ylim=ax.get_ylim()\n",
    "\n",
    "# MFTAN_bg = np.array(bathy_df.slope_list)\n",
    "# bg_threshold = 30\n",
    "# MFTAN_bg[MFTAN_bg>bg_threshold] = bg_threshold\n",
    "# MFTAN_bg[0] = 3*np.nanmax(MFTAN_bg)\n",
    "\n",
    "if plt_bg:\n",
    "    sns.scatterplot(\n",
    "        bathy_df.utm_x_list - dbd_utm_x,\n",
    "        bathy_df.utm_y_list - dbd_utm_y,\n",
    "        MFTAN_bg,\n",
    "        marker='s',\n",
    "        ax=ax,\n",
    "        s=200,\n",
    "#         s=80,\n",
    "#         s=20,\n",
    "        palette=\"gray_r\",\n",
    "        linewidth=0,\n",
    "        zorder=0,\n",
    "        legend=False,\n",
    "    )\n",
    "ax.set_xlim(xlim)\n",
    "ax.set_ylim(ylim)\n",
    "plt.xlabel('X Position [m]')\n",
    "plt.ylabel('Y Position [m]')\n",
    "plt.suptitle('MF-TAN: Dive %s '%dive_label +'-- Map Var Resolution: %s'%map_var_resolution,  fontweight='bold')\n",
    "#plt.savefig('/Users/zduguid/Desktop/fig/tmp.png')\n",
    "#plt.close()\n",
    "#print('> Done plotting!')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='navigation-performance'></a>\n",
    "### B. Navigation Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantify Performance of DR-DACC, DVL-ODO, and MF-TAN\n",
    "\n",
    "#%% Sanity Checks\n",
    "time_plot = ts.df.time - ts.df.time[0]\n",
    "plt.figure()\n",
    "plt.plot(time_plot, tan_pos_r)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Position Uncertainty [m]')\n",
    "plt.title('Sanity Check')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(time_plot, -1*ts.df.depth)\n",
    "plt.xlabel('Time [s]')\n",
    "plt.ylabel('Depth [m]')\n",
    "plt.title('Sanity Check')\n",
    "\n",
    "\n",
    "#%% Extract last gps fix before start of dive and first gps fix when surfacing from a dive\n",
    "surface_flag = False\n",
    "depth_cutoff = 5\n",
    "DBD_start_stop_points_idx = []\n",
    "for i in range(1, len(df_dbd.m_depth) - 2):\n",
    "    if not np.isnan(df_dbd.m_gps_x_lmc[i]) and surface_flag is False:\n",
    "        surface_flag = True\n",
    "        DBD_start_stop_points_idx.append(i)\n",
    "    elif not np.isnan(df_dbd.m_gps_x_lmc[i]) and np.isnan(df_dbd.m_gps_x_lmc[i+1]) and np.isnan(df_dbd.m_gps_x_lmc[i+2]) and surface_flag is True:\n",
    "            DBD_start_stop_points_idx.append(i)\n",
    "            surface_flag = False\n",
    "        \n",
    "# print(DBD_start_stop_points_idx)\n",
    "plt.figure()\n",
    "plt.plot(df_dbd.m_gps_x_lmc[DBD_start_stop_points_idx] - dbd_origin_x_lmc,df_dbd.m_gps_y_lmc[DBD_start_stop_points_idx] - dbd_origin_y_lmc, '*')\n",
    "plt.plot(df_dbd.m_gps_x_lmc - dbd_origin_x_lmc, df_dbd.m_gps_y_lmc - dbd_origin_y_lmc, '+')\n",
    "plt.legend(['start_stop GPS points', 'All GPS fixes'])\n",
    "plt.title('Picking out start_stop GPS fixes')\n",
    "\n",
    "DBD_start_stop_points_idx_cut = DBD_start_stop_points_idx[1:-1]\n",
    "plt.figure()\n",
    "plt.plot(df_dbd.m_gps_x_lmc[DBD_start_stop_points_idx_cut] - dbd_origin_x_lmc,df_dbd.m_gps_y_lmc[DBD_start_stop_points_idx_cut] - dbd_origin_y_lmc, '*', markersize=10)\n",
    "plt.plot(df_dbd.m_gps_x_lmc - dbd_origin_x_lmc, df_dbd.m_gps_y_lmc - dbd_origin_y_lmc, '+')\n",
    "plt.legend(['start_stop GPS points', 'All GPS fixes'])\n",
    "plt.title('Picking out start_stop GPS fixes (cropped)')\n",
    "\n",
    "#%% Reconcile Glider Log timestamp to DVL timestamp and extract DVL-ODO and MF-TAN Nav start_stop points\n",
    "DVL_start_stop_times = []\n",
    "for idx in DBD_start_stop_points_idx_cut:\n",
    "    DVL_start_stop_times.append(datetime.datetime.fromtimestamp(df_dbd.time[idx])) \n",
    "# print(DVL_start_stop_times)\n",
    "\n",
    "DBD_start_stop_points_idx_cut = DBD_start_stop_points_idx[1:-1]\n",
    "# print(DBD_start_stop_points_idx)\n",
    "# print(DBD_start_stop_points_idx_cut)\n",
    "nav = {\n",
    "    'dive' : {\n",
    "        'gps_x' : [],\n",
    "        'gps_y' : [],\n",
    "        \n",
    "        'dac_x' : [],\n",
    "        'dac_y' : [],\n",
    "        \n",
    "        'odo_x' : [],\n",
    "        'odo_y' : [],\n",
    "        \n",
    "        'tan_x' : [],\n",
    "        'tan_y' : [],\n",
    "    }\n",
    "}\n",
    "\n",
    "origin_x = dbd_origin_x_lmc\n",
    "origin_y = dbd_origin_y_lmc\n",
    "\n",
    "for idx in DBD_start_stop_points_idx_cut:\n",
    "    nav['dive']['gps_x'].append(df_dbd.m_gps_x_lmc[idx] - origin_x)\n",
    "    nav['dive']['gps_y'].append(df_dbd.m_gps_y_lmc[idx] - origin_y)\n",
    "dac_delay = 1\n",
    "for i in range(0, len(DBD_start_stop_points_idx_cut)):\n",
    "    if i %2 == 0:\n",
    "        nav['dive']['dac_x'].append(df_dbd.m_x_lmc[DBD_start_stop_points_idx_cut[i]] - origin_x)\n",
    "        nav['dive']['dac_y'].append(df_dbd.m_y_lmc[DBD_start_stop_points_idx_cut[i]] - origin_y)\n",
    "    else:\n",
    "        if not np.isnan(df_dbd.m_x_lmc[DBD_start_stop_points_idx_cut[i]-dac_delay] - origin_x):\n",
    "            dac_x_temp = df_dbd.m_x_lmc[DBD_start_stop_points_idx_cut[i]-dac_delay] - origin_x\n",
    "            dac_y_temp = df_dbd.m_y_lmc[DBD_start_stop_points_idx_cut[i]-dac_delay] - origin_y\n",
    "        else:\n",
    "            for j in range(DBD_start_stop_points_idx_cut[i]-dac_delay, 0, -1):\n",
    "                if not np.isnan(df_dbd.m_x_lmc[j] - origin_x):\n",
    "                    dac_x_temp = df_dbd.m_x_lmc[j] - origin_x\n",
    "                    dac_y_temp = df_dbd.m_y_lmc[j] - origin_y\n",
    "                    break\n",
    "        nav['dive']['dac_x'].append(dac_x_temp)\n",
    "        nav['dive']['dac_y'].append(dac_y_temp)\n",
    "    \n",
    "DVL_start = 0\n",
    "DVL_end = -10\n",
    "for i in range(1, len(DVL_start_stop_times),2):\n",
    "    ts_temp = (ts.df[str(DVL_start_stop_times[i-1]):str(DVL_start_stop_times[i])].copy())\n",
    "    nav['dive']['odo_x'].append(ts_temp.rel_pos_x[DVL_start])\n",
    "    nav['dive']['odo_y'].append(ts_temp.rel_pos_y[DVL_start])\n",
    "    nav['dive']['odo_x'].append(ts_temp.rel_pos_x[DVL_end])\n",
    "    nav['dive']['odo_y'].append(ts_temp.rel_pos_y[DVL_end])\n",
    "    nav['dive']['tan_x'].append(ts_temp.tan_pos_x[DVL_start])\n",
    "    nav['dive']['tan_y'].append(ts_temp.tan_pos_y[DVL_start])\n",
    "    nav['dive']['tan_x'].append(ts_temp.tan_pos_x[DVL_end])\n",
    "    nav['dive']['tan_y'].append(ts_temp.tan_pos_y[DVL_end])\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(df_dbd.m_gps_x_lmc - dbd_origin_x_lmc, df_dbd.m_gps_y_lmc - dbd_origin_y_lmc, '+')\n",
    "plt.plot(df_dbd.m_x_lmc - dbd_origin_x_lmc, df_dbd.m_y_lmc - dbd_origin_y_lmc)\n",
    "plt.plot(ts.df.rel_pos_x, ts.df.rel_pos_y)\n",
    "plt.plot(ts.df.tan_pos_x, ts.df.tan_pos_y)\n",
    "plt.legend(['GPS', \"DR\", \"DVL-ODO\", \"MF-TAN\"])\n",
    "plt.title('Sanity Check')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(nav['dive']['gps_x'], nav['dive']['gps_y'], 'k+' , markersize=10)\n",
    "plt.plot(nav['dive']['dac_x'], nav['dive']['dac_y'], 'b*', markersize=6 )\n",
    "plt.plot(nav['dive']['odo_x'], nav['dive']['odo_y'], 'o', markersize=3 )\n",
    "plt.plot(nav['dive']['tan_x'], nav['dive']['tan_y'], 'r*', markersize=6 )\n",
    "\n",
    "plt.legend(['GPS', 'DR', \"DVL-ODO\", \"MF-TAN\"])\n",
    "plt.title('Start_stop points')\n",
    "\n",
    "\n",
    "print('DAC_x: ', nav['dive']['dac_x'])\n",
    "print('DAC_y: ', nav['dive']['dac_y'])\n",
    "print('GPS_x: ', nav['dive']['gps_x'])\n",
    "print('GPS_y: ', nav['dive']['gps_y'])\n",
    "# print('DVL_odo_x: ', nav['dive']['odo_x'])\n",
    "# print('DVL_odo_y: ', nav['dive']['odo_y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbd_utm_x, dbd_utm_y, _, zone_letter = get_utm_coords_from_glider_lat_lon(\n",
    "    dbd_origin_m_lat, \n",
    "    dbd_origin_m_lon\n",
    ")\n",
    "\n",
    "print(dbd_utm_y)\n",
    "print(dbd_utm_x)\n",
    "print(_)\n",
    "print(zone_letter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='print_metrics'></a>\n",
    "### Print Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nav_range = [] \n",
    "dac_error = []\n",
    "odo_error = []\n",
    "tan_error = []\n",
    "\n",
    "## Needed for only dive A --> to fix, would need to automate counting how many actual dives \"legs\" there are\n",
    "for leg in range(1,2): \n",
    "# for leg in range(1, len(nav['dive']['gps_x']),2):\n",
    "    delta_gps_x = nav['dive']['gps_x'][leg] - nav['dive']['gps_x'][leg-1]\n",
    "    delta_gps_y = nav['dive']['gps_y'][leg] - nav['dive']['gps_y'][leg-1]\n",
    "    delta_gps   = np.linalg.norm([delta_gps_x, delta_gps_y])\n",
    "    nav_range.append(delta_gps)\n",
    "\n",
    "    delta_dac_x = nav['dive']['dac_x'][leg] - nav['dive']['gps_x'][leg]\n",
    "    delta_dac_y = nav['dive']['dac_y'][leg] - nav['dive']['gps_y'][leg]\n",
    "    delta_dac   = np.linalg.norm([delta_dac_x, delta_dac_y])\n",
    "    dac_error.append(delta_dac)\n",
    "\n",
    "    delta_odo_x = nav['dive']['odo_x'][leg] - nav['dive']['gps_x'][leg]\n",
    "    delta_odo_y = nav['dive']['odo_y'][leg] - nav['dive']['gps_y'][leg]\n",
    "    delta_odo   = np.linalg.norm([delta_odo_x, delta_odo_y])\n",
    "    odo_error.append(delta_odo)\n",
    "\n",
    "    delta_tan_x = nav['dive']['tan_x'][leg] - nav['dive']['gps_x'][leg]\n",
    "    delta_tan_y = nav['dive']['tan_y'][leg] - nav['dive']['gps_y'][leg]\n",
    "    delta_tan   = np.linalg.norm([delta_tan_x, delta_tan_y])\n",
    "    tan_error.append(delta_tan)\n",
    "\n",
    "mission_range = sum(nav_range)\n",
    "print('Dive: ', dive_label)\n",
    "print('  Range:   %3d'   % mission_range)\n",
    "print('  DR-DACC: %0.1f' % (sum(dac_error)/sum(nav_range)*100))\n",
    "print('  DVL-Odo: %0.1f' % (sum(odo_error)/sum(nav_range)*100))\n",
    "print('  MF-TAN:  %0.1f' % (sum(tan_error)/sum(nav_range)*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='sandbox'></a>\n",
    "# Sandbox\n",
    "Code below this cell is experimental and may contain bugs and/or may be garbage code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check for UTM estimate used to compare to map\n",
    "# plt.figure()\n",
    "# plt.plot(utm_est_x_list, utm_est_y_list)\n",
    "# plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(depth_var_list)\n",
    "# plt.title('Depth TAU')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(slope_var_list)\n",
    "# plt.title('Slope TAU')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(orient_var_list)\n",
    "# plt.title('Orientation TAU')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plots of Factors used for fixes\n",
    "# plt.figure()\n",
    "# plt.plot(tan_update_depth)\n",
    "# plt.title('depth')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(tan_update_slope)\n",
    "# plt.title('slope')\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(tan_update_orient)\n",
    "# plt.title('orient')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drag Coefficient Estimation\n",
    "Pump, thruster, Measured Vog, Pitch, rate of ascent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure()\n",
    "# plt.plot(df_dbd.m_thruster_current,'*')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drag_est_data = {\n",
    "#     'time'             : df_dbd.m_present_time,\n",
    "#     'pitch'            : df_dbd.m_pitch,\n",
    "#     'depth'            : df_dbd.m_depth,\n",
    "#     'depth_rate'       : df_dbd.m_depth_rate,\n",
    "#     'v_N'              : df_dbd.m_vx_lmc,\n",
    "#     'v_E'              : df_dbd.m_vy_lmc,\n",
    "#     'pump_volume'      : df_dbd.m_de_oil_vol,\n",
    "#     'thruster_power'   : df_dbd.m_thruster_power,\n",
    "#     'thruster_current' : df_dbd.m_thruster_current,\n",
    "#     }\n",
    "# drag_est_df = pd.DataFrame.from_dict(drag_est_data)\n",
    "\n",
    "# drag_est_df.to_csv('drag_est_data_dive_g.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ocean Current Extraction for Kolumbo Dives\n",
    "\n",
    "GOAL: function that extracts time, x_lmc, y_lmc, z(depth), and the ocean current vector experienced by the vehicle in the NED frame\n",
    "\n",
    "*should be able to just change the timeseries for each dive and produce it for all dives and the output should be in a dataframe so it can be easily saved to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all of Ocean Current Data\n",
    "ocean_current_log = {}\n",
    "for t in range(1,len(ts.df.time)):\n",
    "# for t in range(1,5):\n",
    "    shear_node_list = []\n",
    "    shear_node = water_column.get_voc_at_time(t)\n",
    "    if type(shear_node) == list:\n",
    "        for i in range(1,len(shear_node)):\n",
    "            shear_node_list.append([shear_node[i].z_true, shear_node[i].z_bin, shear_node[i].voc.u,shear_node[i].voc.v])\n",
    "    else:\n",
    "       shear_node_list = []\n",
    "    ocean_current_log[t] = [ts.df.depth[t], ts.df.rel_pos_x[t], ts.df.rel_pos_y[t], shear_node_list]\n",
    "\n",
    "#Extract time averaged ocean current at each depth bin \n",
    "voc_u_list,voc_v_list,voc_w_list,voc_z_list = water_column.compute_averages()\n",
    "\n",
    "avg_ocean_currents= {\n",
    "    'depth'   : voc_z_list,\n",
    "    'u_vel'   : voc_u_list,\n",
    "    'v_vel'   : voc_v_list,\n",
    "    'w_vel'   : voc_w_list,\n",
    "}\n",
    "\n",
    "#Save dictionaries to dataframes and then to csv files\n",
    "\n",
    "ocean_current_df = pd.DataFrame.from_dict(ocean_current_log)\n",
    "avg_ocean_current_df = pd.DataFrame.from_dict(avg_ocean_currents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ocean_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ocean_current_df.to_csv('ocean_current_full_dive_g.csv', sep=',', index=False)\n",
    "# avg_ocean_current_df.to_csv('ocean_current_avg_dive_g.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract time, depth, bin #, u velocity, and v velocity for all ocean current measurements\n",
    "\n",
    "TODO Associated \"exact bin number\" --> REMOVE 1st and 2nd bin filter\n",
    "--> right now: first 2 and last 2 are hardcoded to be filtered out\n",
    "--> other consideration is that DVL will discard bin if it is not coherent enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract all of Ocean Current Data\n",
    "for t in range(1,len(ts.df.time)):\n",
    "# for t in range(1,5):\n",
    "    shear_node = water_column.get_voc_at_time(t)\n",
    "    if type(shear_node) == list:\n",
    "        for i in range(1,len(shear_node)):\n",
    "            ocean_current_log.append([t, shear_node[i].z_true, shear_node[i].z_bin, shear_node[i].voc.u,shear_node[i].voc.v])\n",
    "    else:\n",
    "       continue\n",
    "\n",
    "#Extract time averaged ocean current at each depth bin \n",
    "# voc_u_list,voc_v_list,voc_w_list,voc_z_list = water_column.compute_averages()\n",
    "\n",
    "# avg_ocean_currents= {\n",
    "#     'depth'   : voc_z_list,\n",
    "#     'u_vel'   : voc_u_list,\n",
    "#     'v_vel'   : voc_v_list,\n",
    "#     'w_vel'   : voc_w_list,\n",
    "# }\n",
    "\n",
    "#Save dictionaries to dataframes and then to csv files\n",
    "\n",
    "ocean_current_df = pd.DataFrame.from_dict(ocean_current_log)\n",
    "# avg_ocean_current_df = pd.DataFrame.from_dict(avg_ocean_currents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_current_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocean_current_df.to_csv('ocean_current_full_dive_a.csv', sep=',', index=False)\n",
    "# avg_ocean_current_df.to_csv('ocean_current_avg_dive_g.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variability of Water Column Velocity Estimates as Measured by the DVL\n",
    "\n",
    "Hypothesis: we expect to see higher variability during inflections.\n",
    "\n",
    "Therefore, when implementing the UKF, we can account for this by increasing our uncertainty of the DVL water column velocity estimates when vehicle is pitched a certain amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timeseries_nodes_sorted = {key:value for key, value in sorted(timeseries_nodes.items(), key=lambda item: int(item[0]))}\n",
    "# print(timeseries_nodes_sorted)\n",
    "# ts_sorted = pd.DataFrame.from_dict(timeseries_nodes_sorted)\n",
    "# plt.figure()\n",
    "# plt.xlabel('Time [s]')\n",
    "# plt.ylabel('Avg Ocean Current Vel [m/s]')\n",
    "\n",
    "# period = 10\n",
    "# for key in timeseries_nodes_sorted:\n",
    "#     sum_cum = 0\n",
    "#     for i in range(1,10):\n",
    "#         sum_cum  = sum_cum + ((timeseries_nodes_sorted[key].u + timeseries_nodes_sorted[key].v)/2)\n",
    "#         if i == 10:\n",
    "#             plt.plot(sum_cum/period, '*k')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
